{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acdd711",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b1fb8",
   "metadata": {},
   "source": [
    "# Optimizing a Video AI Application #\n",
    "The effectiveness of a video AI application will largely depend on the inference performance of the video AI model(s). Thus far we have been able to train a video AI model with the TAO Toolkit, but we have not considered the inference performance. This is an important consideration to ensure the DeepStream pipeline runs smoothly and without delays. Furthermore, this will allow the video AI application to be deployed on edge devices that have less computational capabilities. A complete model training workflow includes optimization after the model has been trained to use powerful features such as pruning and quantization before deployment. \n",
    "\n",
    "<img src='images/optimized_pre-trained_model_workflow.png' width=1080>\n",
    "\n",
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to use the TAO Toolkit to optimize a model for inference performance, including: \n",
    "* Building a Multi-source DeepStream Pipeline\n",
    "* Fine-Tuning a Video AI Model for Deployment to DeepStream\n",
    "* Pruning a Trained Detectnet_v2 Model\n",
    "* Using Quantization-Aware Training\n",
    "\n",
    "**Table of Contents** \n",
    "<br>\n",
    "This notebook covers the below sections: \n",
    "1. [Multi-source DeepStream Pipeline](#s1)\n",
    "    * [Exercise #1 - Build a DeepStream Pipeline with Multiple Sources](#e1)\n",
    "2. [Optimizing Video AI Model for Inference](#s2)\n",
    "    * [Model Pruning](#s2.1)\n",
    "    * [Evaluate Pruned Model](#s2.2)\n",
    "    * [Exercise #2 - Model Comparison](#e2)\n",
    "    * [Retrain Pruned Model with Quantization-Aware Training](#s2.3)\n",
    "    * [Exercise #3 - Convert Pruned Model to QAT and Retrain](#e3)\n",
    "3. [Evaluate Retrained Model](#s3)\n",
    "4. [Export with Calibration Cache](#s4)\n",
    "5. [Deployment to DeepStream](#s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7737d5-9b80-471e-9a31-c59f102b20c8",
   "metadata": {},
   "source": [
    "Execute the below cell to set directories for the TAO Toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78daac3f-841b-4f3e-a454-03e3e2a2d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set and create directories for the TAO Toolkit experiment\n",
    "import os\n",
    "\n",
    "#!mkdir logs\n",
    "os.environ['PROJECT_DIR']='/dli/task/tao_project'\n",
    "os.environ['SOURCE_DATA_DIR']='/dli/task/data'\n",
    "os.environ['DATA_DIR']='/dli/task/tao_project/data'\n",
    "os.environ['MODELS_DIR']='/dli/task/tao_project/models'\n",
    "os.environ['SPEC_FILES_DIR']='/dli/task/spec_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331d1bf-f438-4b38-a6f7-628ee96f256f",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## Multi-source DeepStream Pipeline ##\n",
    "The DeepStream SDK enables building a pipeline with multiple input video streams. When there are multiple input sources, each source must have its own decoder and be linked to the `Gst-nvstreammux`. The `Gst-nvstreammux` plugin, referred to as the **muxer**, forms a batch of frames from multiple input sources. When connecting a source to the muxer, a new pad must be requested from the muxer using `get_request_pad()` with the pad template `sink_%u`. The muxer will form a batched buffer with `<batch-size>` frames, which is specified using `set_property()`. If the muxer’s output format and input format are the same, the muxer forwards the frames from that source as a part of the muxer’s output batched buffer. If the resolutions are not the same, the mux scales frames from the input into the batched buffer. The muxer maintains that all frames in the batch have the same resolution when it pushes it downstream. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac59a4-227c-4d0e-a8a4-e43b15c3f1a5",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Build a DeepStream Pipeline with Multiple Sources ####\n",
    "To demonstrate a DeepStream pipeline with multiple inputs, we created a sample application [app_04.py](sample_apps/app_04.py) with the below architecture. This pipeline is very similar to the pipelines we've built so far with a few modifications: \n",
    "1. It takes _one_ video file and uses it for an arbitrary number of file sources (`filesrc`). \n",
    "2. It uses a tiler (`Gst-nvmultistreamtiler`) to composite a 2D tile from batched buffers, which needs to have the `rows`, `columns`, `width`, and `height` properties set. \n",
    "3. It uses the Object Detection model we had built in the previous notebook. \n",
    "4. The probe callback function is attached to the source pad of the tiler. \n",
    "\n",
    "We can run the pipeline by executing the script and passing 4 arguments as: <br> `python sample_apps/app_04.py <path to input h264 video> <path to nvinfer config file> <number of file sources> <name of output file>`. \n",
    "\n",
    "<p><img src='images/multi_input_pipeline.png' width=1080></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bea01-881b-4534-a201-c3d5e71f3565",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Review the code for [app_04.py](sample_apps/app_04.py). \n",
    "* Modify the `<FIXME>`s only to create the necessary elements that will connect to the `Gst-nvstreammux`, iteratively based on the arguments passed. Please **save changes** to the file. \n",
    "* Execute the below cells to review the nvinfer config file, run the DeepStream pipeline, and view the `nvdia-smi` log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22173b0-bb9e-4fd5-9a32-2a9e7f2e1f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /dli/task/spec_files/pgie_config_trafficcamnet_retrained.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the nvinfer config file\n",
    "!cat $SPEC_FILES_DIR/pgie_config_trafficcamnet_retrained.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db69f1c-9204-4886-9a93-f96b1fe9e7c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pipeline\n",
      "Warning: 'input-dims' parameter has been deprecated. Use 'infer-dims' instead.\n",
      "WARNING: Overriding infer-config batch-size 1  with number of sources  8\n",
      "Adding elements to Pipeline\n",
      "Linking elements in the Pipeline\n",
      "Now playing...\n",
      "1 :  /dli/task/data/sample_30.h264\n",
      "2 :  /dli/task/data/sample_30.h264\n",
      "3 :  /dli/task/data/sample_30.h264\n",
      "4 :  /dli/task/data/sample_30.h264\n",
      "5 :  /dli/task/data/sample_30.h264\n",
      "6 :  /dli/task/data/sample_30.h264\n",
      "7 :  /dli/task/data/sample_30.h264\n",
      "8 :  /dli/task/data/sample_30.h264\n",
      "Starting pipeline\n",
      "0:00:00.279269394 \u001b[335m 1918\u001b[00m      0x350ed30 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:635:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::initialize() <nvdsinfer_context_impl.cpp:1161> [UID = 1]: Warning, OpenCV has been deprecated. Using NMS for clustering instead of cv::groupRectangles with topK = 20 and NMS Threshold = 0.5\n",
      "0:00:00.279386693 \u001b[335m 1918\u001b[00m      0x350ed30 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() <nvdsinfer_context_impl.cpp:1914> [UID = 1]: Trying to create engine from model files\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "WARNING: [TRT]: Detected invalid timing cache, setup a local cache instead\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "0:00:18.241011005 \u001b[335m 1918\u001b[00m      0x350ed30 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() <nvdsinfer_context_impl.cpp:1947> [UID = 1]: serialize cuda engine to file: /dli/task/tao_project/models/resnet18_detector_unpruned/resnet18_detector.etlt_b8_gpu0_fp32.engine successfully\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "INFO: ../nvdsinfer/nvdsinfer_model_builder.cpp:610 [Implicit Engine Info]: layers num: 3\n",
      "0   INPUT  kFLOAT input_1         3x696x888       \n",
      "1   OUTPUT kFLOAT output_bbox/BiasAdd 4x44x56         \n",
      "2   OUTPUT kFLOAT output_cov/Sigmoid 1x44x56         \n",
      "\n",
      "0:00:18.249137489 \u001b[335m 1918\u001b[00m      0x350ed30 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:<primary-inference>\u001b[00m [UID 1]: Load new model:/dli/task/spec_files/pgie_config_resnet18_detector_unpruned.txt sucessfully\n",
      "FPS: 0.43 @ Frame 0.\n",
      "FPS: 72.66 @ Frame 100.\n",
      "FPS: 66.31 @ Frame 200.\n",
      "FPS: 71.48 @ Frame 300.\n",
      "FPS: 65.55 @ Frame 400.\n",
      "FPS: 74.01 @ Frame 500.\n",
      "FPS: 91.39 @ Frame 600.\n",
      "FPS: 68.37 @ Frame 700.\n",
      "End-of-stream--- 82.80774760246277 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the app_04.py DeepStream pipeline w/ the custom ResNet18 model\n",
    "!nvidia-smi dmon -i 0 \\\n",
    "                 -s ucmt \\\n",
    "                 -c 20 > '/dli/task/logs/smi.log' & \\\n",
    "python sample_apps/app_04.py /dli/task/data/sample_30.h264 \\\n",
    "                            /dli/task/spec_files/pgie_config_resnet18_detector_unpruned.txt \\\n",
    "                            8 \\\n",
    "                            output_tiled.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e2c758-d721-4ff4-b2cd-53121c933f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert the output video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_tiled.mp4 output_tiled_conv.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e4e3b4-17cc-489a-8582-197d622f174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_tiled_conv.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Show video\n",
    "from IPython.display import Video\n",
    "Video('output_tiled_conv.mp4', width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6942d0b0-572c-49fb-820a-e561b19ae668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gpu    sm   mem   enc   dec  mclk  pclk    fb  bar1 rxpci txpci\n",
      "# Idx     %     %     %     %   MHz   MHz    MB    MB  MB/s  MB/s\n",
      "    0     0     0     0     0   405   300     0     2     0     0\n",
      "    0     1     0     0     0  5000   585   256     5     0     0\n",
      "    0     5     0     0     0  5000   585   406     5   326    43\n",
      "    0     6     0     0     0  5000   585  1068     5   334    54\n",
      "    0    96    49     0     0  5000  1470  1374     5    36    12\n",
      "    0    96    90     0     0  5000  1545  1374     5    25     7\n",
      "    0    93    37     0     0  5000  1275  1374     5    22     2\n",
      "    0    76    55     0     0  5000  1440  1374     5    41     8\n",
      "    0    88    34     0     0  5000  1365  1374     5   144    25\n",
      "    0    90    48     0     0  5000  1410  1374     5   264    16\n",
      "    0    90    12     0     0  5000  1440  1374     5     3    59\n",
      "    0    90    19     0     0  5000  1365  1374     5     5    59\n",
      "    0    87    38     0     0  5000  1320  1374     5     3    58\n",
      "    0    80    28     0     0  5000  1590  1374     5     0     0\n",
      "    0     0     0     0     0  5000  1590  1374     5     0     0\n",
      "    0     0     0     0     0  5000   585  1374     5     0     0\n",
      "    0     0     0     0     0  5000   585  1374     5     2     0\n",
      "    0    18     3     0     0  5000   585   870     5     0     0\n",
      "    0   100    48     0     2  5000  1020  1808     6  3190   440\n",
      "    0   100    46     0     3  5000  1050  1808     6  2253   293\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the smi.log\n",
    "!cat logs/smi.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedda41-348e-495e-9368-548514ae5e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(number_sources): \n",
    "    print('Creating source_bin ', i, end='\\r')\n",
    "    source=Gst.ElementFactory.make('filesrc', 'file-source_%u'%i)\n",
    "    source.set_property('location', args[1])\n",
    "    h264parser=Gst.ElementFactory.make('h264parse', 'h264-parser_%u'%i)\n",
    "    decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder_%u\"%i)\n",
    "    pipeline.add(source)\n",
    "    pipeline.add(h264parser)\n",
    "    pipeline.add(decoder)\n",
    "    padname=\"sink_%u\"%i\n",
    "    source.link(h264parser)\n",
    "    h264parser.link(decoder)\n",
    "    decodersrcpad=decoder.get_static_pad(\"src\").link(streammux.get_request_pad(padname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3ebbc-6e68-402f-ad88-0c94e2146349",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6682e-8547-4bf3-9a09-5ba66c3d04ae",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "When we process multiple input streams using our current unpruned model, the DeepStream pipeline begins to suffer in performance. \n",
    "1. At the bottom of the output from the pipeline run, we see that it took a while to run the 24 seconds clip, which is significantly longer than it took for a single input. The pipeline processed less than 30 frames per second, which is what the input streams are taken at. This would result in a significant delay if they were live. See [GStreamer's Design Document on Blocking Probe](https://gstreamer.freedesktop.org/documentation/additional/design/probes.html?gi-language=c#blocking-probes) to find out more about why a delay will occur. \n",
    "2. We also saw in the `nvidia-smi` log that the Streaming Multiprocessor is at very high utilization for the duration of the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7eff4d-bbfd-429f-ba67-9a705f33124c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name='s2'></a>\n",
    "## Optimizing Video Model for Inference ##\n",
    "The TAO Toolkit offers several features to optimize a model for inference performance, including **pruning** and **quantization**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a8f0a-9142-4a39-9c14-fea6834ce506",
   "metadata": {},
   "source": [
    "<a name='s2.1'></a>\n",
    "### Model Pruning ###\n",
    "Pruning is one way to fine-tune a model for better inference performance. It is one of the key differentiators for the TAO Toolkit, which involves algorithmically removing neurons from the neural network that do not contribute significantly to the overall accuracy. Pruning reduces the overall size of the model significantly, resulting in a much lower memory footprint and higher inference throughput, which are very important for edge deployment. The model pruning step will inadvertently reduce the accuracy of the model. So after pruning, the next step is to retrain the model on the same data set to recover the lost accuracy. \n",
    "\n",
    "<p><img src='images/pruning.svg' width=540></p>\n",
    "\n",
    "More information about pruning can be found in this [NVIDIA Developer Blog](https://developer.nvidia.com/blog/transfer-learning-toolkit-pruning-intelligent-video-analytics/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e10c6-1233-4655-bbb7-19e8613d6bd1",
   "metadata": {},
   "source": [
    "When using the `prune` subtask, the `-m` argument indictates the path to the pre-trained model, the `-o` argument indictates the path to the output file, and the `-k` argument indictates the key to _load_ the model. Some optional arguments include: \n",
    "* `-eq, --equalization_criterion`: Criteria _(arithmetic_mean, geometric_mean, union (default), and intersection)_ to equalize the states of inputs to an element-wise op layer or depth-wise convolutional layer. This parameter is useful for _ResNets_ and _MobileNets_. \n",
    "* `-pg, --pruning_granularity`: Number of filters to remove at a time _(default=8)_. \n",
    "* `-pth`: Threshold to compare the normalized norm against _(default=0.1)_.\n",
    "* `-nf, --min_num_filters`: Minimum number of filters to keep per layer _(default=16)_. \n",
    "* `-el, --excluded_layers`: List of excluded_layers _(default=[])_. \n",
    "\n",
    "Usually, we just need to adjust `-pth` (threshold) for accuracy and model size trade off. Higher `pth` gives smaller model (and thus higher inference speed) but worse accuracy. The threshold to use depends on the data set. A `pth` value of _0.1_ is just a starting point. If the retrain accuracy is good, we can increase this value to get smaller models. Otherwise, lower this value to get better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da59b4ad-03f7-43af-a3e3-8482e483fd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "usage: detectnet_v2 prune [-h] [--num_processes NUM_PROCESSES] [--gpus GPUS]\n",
      "                          [--gpu_index GPU_INDEX [GPU_INDEX ...]] [--use_amp]\n",
      "                          [--log_file LOG_FILE] -m MODEL -o OUTPUT_FILE -k KEY\n",
      "                          [-n NORMALIZER] [-eq EQUALIZATION_CRITERION]\n",
      "                          [-pg PRUNING_GRANULARITY] [-pth PRUNING_THRESHOLD]\n",
      "                          [-nf MIN_NUM_FILTERS]\n",
      "                          [-el [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]]] [-v]\n",
      "                          {calibration_tensorfile,dataset_convert,evaluate,export,inference,prune,train}\n",
      "                          ...\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --num_processes NUM_PROCESSES, -np NUM_PROCESSES\n",
      "                        The number of horovod child processes to be spawned.\n",
      "                        Default is -1(equal to --gpus).\n",
      "  --gpus GPUS           The number of GPUs to be used for the job.\n",
      "  --gpu_index GPU_INDEX [GPU_INDEX ...]\n",
      "                        The indices of the GPU's to be used.\n",
      "  --use_amp             Flag to enable Auto Mixed Precision.\n",
      "  --log_file LOG_FILE   Path to the output log file.\n",
      "  -m MODEL, --model MODEL\n",
      "                        Path to the target model for pruning\n",
      "  -o OUTPUT_FILE, --output_file OUTPUT_FILE\n",
      "                        Output file path for pruned model\n",
      "  -k KEY, --key KEY     Key to load a .tlt model\n",
      "  -n NORMALIZER, --normalizer NORMALIZER\n",
      "                        `max` to normalize by dividing each norm by the\n",
      "                        maximum norm within a layer; `L2` to normalize by\n",
      "                        dividing by the L2 norm of the vector comprising all\n",
      "                        kernel norms. (default: `max`)\n",
      "  -eq EQUALIZATION_CRITERION, --equalization_criterion EQUALIZATION_CRITERION\n",
      "                        Criteria to equalize the stats of inputs to an element\n",
      "                        wise op layer. Options are [arithmetic_mean,\n",
      "                        geometric_mean, union, intersection]. (default:\n",
      "                        `union`)\n",
      "  -pg PRUNING_GRANULARITY, --pruning_granularity PRUNING_GRANULARITY\n",
      "                        Pruning granularity: number of filters to remove at a\n",
      "                        time. (default:8)\n",
      "  -pth PRUNING_THRESHOLD, --pruning_threshold PRUNING_THRESHOLD\n",
      "                        Threshold to compare normalized norm against\n",
      "                        (default:0.1)\n",
      "  -nf MIN_NUM_FILTERS, --min_num_filters MIN_NUM_FILTERS\n",
      "                        Minimum number of filters to keep per layer.\n",
      "                        (default:16)\n",
      "  -el [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]], --excluded_layers [EXCLUDED_LAYERS [EXCLUDED_LAYERS ...]]\n",
      "                        List of excluded_layers. Examples: -i item1 item2\n",
      "  -v, --verbose         Include this flag in command line invocation for\n",
      "                        verbose logs.\n",
      "\n",
      "tasks:\n",
      "  {calibration_tensorfile,dataset_convert,evaluate,export,inference,prune,train}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# View prune usage\n",
    "!detectnet_v2 prune --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1578fe8a-d984-4083-a5e5-5b1bee28c5eb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "2022-12-23 02:55:30,250 [INFO] modulus.pruning.pruning: Exploring graph for retainable indices\n",
      "2022-12-23 02:55:30,837 [INFO] modulus.pruning.pruning: Pruning model and appending pruned nodes to new graph\n",
      "2022-12-23 02:55:46,729 [INFO] iva.common.magnet_prune: Pruning ratio (pruned model / original model): 0.15538467817115237\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create a new ResNet model folder and prune the resnet18_detector model\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_pruned\n",
    "!mkdir -p $MODELS_DIR/resnet18_detector_pruned\n",
    "\n",
    "!detectnet_v2 prune -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                    -o $MODELS_DIR/resnet18_detector_pruned/resnet18_detector_pruned.tlt \\\n",
    "                    -k tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3571c-d038-4a10-bd1d-361f4a6f30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List the model and sizes\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector/weights\n",
    "\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b83c96-ff56-4b5e-882b-59f743fb9d73",
   "metadata": {},
   "source": [
    "<a name='s2.22'></a>\n",
    "### Evaluate Pruned Model ###\n",
    "Once the model has been pruned, there can be a decrease in accuracy because some previously useful weights may have been removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c83d6-1849-421f-a624-801862ddf892",
   "metadata": {},
   "source": [
    "Execute the below cells to compare unpruned model evaluation with that of the pruned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079bbcef-7988-44eb-b05a-b51bbd80c3a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "2022-12-23 02:56:05,279 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /dli/task/spec_files/combined_training_config.txt\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-12-23 02:56:05,283 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-12-23 02:56:05,653 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-12-23 02:56:05,662 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-12-23 02:56:05,684 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-12-23 02:56:06,466 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2022-12-23 02:56:06,466 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-12-23 02:56:06,466 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-12-23 02:56:06,927 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-12-23 02:56:06,927 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-12-23 02:56:07,337 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2022-12-23 02:56:07,748 [INFO] iva.detectnet_v2.objectives.bbox_objective: Default L1 loss function will be used.\n",
      "2022-12-23 02:56:08,023 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-12-23 02:56:08,023 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-12-23 02:56:08,023 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-12-23 02:56:08,023 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: 4\n",
      "2022-12-23 02:56:08,023 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 578, number of sources: 1, batch size per gpu: 16, steps: 37\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "2022-12-23 02:56:08,057 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fb2a8f9a588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fb2a8f9a588>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:56:08,095 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fb2a8f9a588>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fb2a8f9a588>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:56:08,115 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-12-23 02:56:08,364 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: False - shard 0 of 1\n",
      "2022-12-23 02:56:08,370 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-12-23 02:56:08,370 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fb2a875ccc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fb2a875ccc0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:56:08,384 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fb2a875ccc0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fb2a875ccc0>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:56:08,636 [INFO] iva.detectnet_v2.evaluation.build_evaluator: Found 578 samples in validation set\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2022-12-23 02:56:08,637 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2022-12-23 02:56:08,637 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-12-23 02:56:08,639 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "2022-12-23 02:56:08,765 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-12-23 02:56:09,152 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-12-23 02:56:09,162 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 544, 960)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 64, 272, 480) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 64, 272, 480) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 272, 480) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (Conv2D)        (None, 64, 136, 240) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 64, 136, 240) 256         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (Activation)    (None, 64, 136, 240) 0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (Conv2D)        (None, 64, 136, 240) 36928       block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Conv2D) (None, 64, 136, 240) 4160        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 136, 240) 256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 136, 240) 0           block_1a_bn_2[0][0]              \n",
      "                                                                 block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (Activation)      (None, 64, 136, 240) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (Conv2D)        (None, 64, 136, 240) 36928       block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (Activation)    (None, 64, 136, 240) 0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (Conv2D)        (None, 64, 136, 240) 36928       block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 136, 240) 0           block_1b_bn_2[0][0]              \n",
      "                                                                 block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (Activation)      (None, 64, 136, 240) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (Conv2D)        (None, 128, 68, 120) 73856       block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 128, 68, 120) 512         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (Activation)    (None, 128, 68, 120) 0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (Conv2D)        (None, 128, 68, 120) 147584      block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Conv2D) (None, 128, 68, 120) 8320        block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 128, 68, 120) 512         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 128, 68, 120) 512         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 68, 120) 0           block_2a_bn_2[0][0]              \n",
      "                                                                 block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (Activation)      (None, 128, 68, 120) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (Conv2D)        (None, 128, 68, 120) 147584      block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 128, 68, 120) 512         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (Activation)    (None, 128, 68, 120) 0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (Conv2D)        (None, 128, 68, 120) 147584      block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 128, 68, 120) 512         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 128, 68, 120) 0           block_2b_bn_2[0][0]              \n",
      "                                                                 block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (Activation)      (None, 128, 68, 120) 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (Conv2D)        (None, 256, 34, 60)  295168      block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 256, 34, 60)  1024        block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (Activation)    (None, 256, 34, 60)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (Conv2D)        (None, 256, 34, 60)  590080      block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Conv2D) (None, 256, 34, 60)  33024       block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 256, 34, 60)  1024        block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 256, 34, 60)  1024        block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 256, 34, 60)  0           block_3a_bn_2[0][0]              \n",
      "                                                                 block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (Activation)      (None, 256, 34, 60)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (Conv2D)        (None, 256, 34, 60)  590080      block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 256, 34, 60)  1024        block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (Activation)    (None, 256, 34, 60)  0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (Conv2D)        (None, 256, 34, 60)  590080      block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 256, 34, 60)  1024        block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 256, 34, 60)  0           block_3b_bn_2[0][0]              \n",
      "                                                                 block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (Activation)      (None, 256, 34, 60)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (Conv2D)        (None, 512, 34, 60)  1180160     block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 512, 34, 60)  2048        block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (Activation)    (None, 512, 34, 60)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (Conv2D)        (None, 512, 34, 60)  2359808     block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Conv2D) (None, 512, 34, 60)  131584      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 512, 34, 60)  2048        block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 512, 34, 60)  2048        block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 512, 34, 60)  0           block_4a_bn_2[0][0]              \n",
      "                                                                 block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (Activation)      (None, 512, 34, 60)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (Conv2D)        (None, 512, 34, 60)  2359808     block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 512, 34, 60)  2048        block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (Activation)    (None, 512, 34, 60)  0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (Conv2D)        (None, 512, 34, 60)  2359808     block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 512, 34, 60)  2048        block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 512, 34, 60)  0           block_4b_bn_2[0][0]              \n",
      "                                                                 block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (Activation)      (None, 512, 34, 60)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_bbox (Conv2D)            (None, 4, 34, 60)    2052        block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_cov (Conv2D)             (None, 1, 34, 60)    513         block_4b_relu[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,197,893\n",
      "Trainable params: 11,026,821\n",
      "Non-trainable params: 171,072\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "2022-12-23 02:56:09,171 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "2022-12-23 02:56:09,171 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "2022-12-23 02:56:09,171 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "2022-12-23 02:56:09,172 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "2022-12-23 02:56:09,172 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2022-12-23 02:56:09,539 [INFO] tensorflow: Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2022-12-23 02:56:09,902 [INFO] tensorflow: Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2022-12-23 02:56:10,237 [INFO] tensorflow: Done running local_init_op.\n",
      "2022-12-23 02:56:10,914 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 37, 0.00s/step\n",
      "2022-12-23 02:56:22,810 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 37, 1.19s/step\n",
      "2022-12-23 02:56:27,501 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 37, 0.47s/step\n",
      "2022-12-23 02:56:32,179 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 37, 0.47s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 4032/4032 [00:00<00:00, 18694.17it/s]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-12-23 02:56:35,914 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-12-23 02:56:35,914 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "\n",
      "Validation cost: 0.000893\n",
      "Mean average_precision (in %): 65.6894\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                              65.6894\n",
      "\n",
      "Median Inference Time: 0.017909\n",
      "2022-12-23 02:56:35,955 [INFO] __main__: Evaluation complete.\n",
      "Time taken to run __main__:main: 0:00:30.677748.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the unpruned model\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector/weights/resnet18_detector.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763b1881-d01c-4364-aaa4-fba454b6e795",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "2022-12-23 02:57:12,621 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /dli/task/spec_files/combined_training_config.txt\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-12-23 02:57:12,625 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-12-23 02:57:12,725 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-12-23 02:57:12,733 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-12-23 02:57:12,756 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-12-23 02:57:13,519 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2022-12-23 02:57:13,519 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-12-23 02:57:13,519 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-12-23 02:57:13,986 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-12-23 02:57:13,987 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-12-23 02:57:14,397 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2022-12-23 02:57:14,761 [INFO] iva.detectnet_v2.objectives.bbox_objective: Default L1 loss function will be used.\n",
      "2022-12-23 02:57:15,054 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-12-23 02:57:15,054 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-12-23 02:57:15,054 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-12-23 02:57:15,054 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: 4\n",
      "2022-12-23 02:57:15,054 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 578, number of sources: 1, batch size per gpu: 16, steps: 37\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "2022-12-23 02:57:15,088 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fd632f5a438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fd632f5a438>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:57:15,126 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fd632f5a438>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fd632f5a438>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:57:15,146 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-12-23 02:57:15,394 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: False - shard 0 of 1\n",
      "2022-12-23 02:57:15,400 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-12-23 02:57:15,400 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fd5b03c6d30>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fd5b03c6d30>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:57:15,414 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fd5b03c6d30>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fd5b03c6d30>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 02:57:15,664 [INFO] iva.detectnet_v2.evaluation.build_evaluator: Found 578 samples in validation set\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2022-12-23 02:57:15,664 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2022-12-23 02:57:15,665 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-12-23 02:57:15,667 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "2022-12-23 02:57:15,793 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-12-23 02:57:16,180 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-12-23 02:57:16,190 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 544, 960)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 24, 272, 480) 3552        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 24, 272, 480) 96          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 272, 480) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (Conv2D)        (None, 48, 136, 240) 10416       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 48, 136, 240) 192         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (Activation)    (None, 48, 136, 240) 0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (Conv2D)        (None, 64, 136, 240) 27712       block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Conv2D) (None, 64, 136, 240) 1600        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 136, 240) 256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 136, 240) 0           block_1a_bn_2[0][0]              \n",
      "                                                                 block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (Activation)      (None, 64, 136, 240) 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (Conv2D)        (None, 64, 136, 240) 36928       block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (Activation)    (None, 64, 136, 240) 0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (Conv2D)        (None, 64, 136, 240) 36928       block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 136, 240) 0           block_1b_bn_2[0][0]              \n",
      "                                                                 block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (Activation)      (None, 64, 136, 240) 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (Conv2D)        (None, 80, 68, 120)  46160       block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 80, 68, 120)  320         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (Activation)    (None, 80, 68, 120)  0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (Conv2D)        (None, 112, 68, 120) 80752       block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Conv2D) (None, 112, 68, 120) 7280        block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 112, 68, 120) 448         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 112, 68, 120) 0           block_2a_bn_2[0][0]              \n",
      "                                                                 block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (Activation)      (None, 112, 68, 120) 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (Conv2D)        (None, 96, 68, 120)  96864       block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 96, 68, 120)  384         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (Activation)    (None, 96, 68, 120)  0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (Conv2D)        (None, 112, 68, 120) 96880       block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 112, 68, 120) 0           block_2b_bn_2[0][0]              \n",
      "                                                                 block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (Activation)      (None, 112, 68, 120) 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (Conv2D)        (None, 104, 34, 60)  104936      block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 104, 34, 60)  416         block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (Activation)    (None, 104, 34, 60)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (Conv2D)        (None, 192, 34, 60)  179904      block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Conv2D) (None, 192, 34, 60)  21696       block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 192, 34, 60)  0           block_3a_bn_2[0][0]              \n",
      "                                                                 block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (Activation)      (None, 192, 34, 60)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (Conv2D)        (None, 96, 34, 60)   165984      block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 96, 34, 60)   384         block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (Activation)    (None, 96, 34, 60)   0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (Conv2D)        (None, 192, 34, 60)  166080      block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 192, 34, 60)  0           block_3b_bn_2[0][0]              \n",
      "                                                                 block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (Activation)      (None, 192, 34, 60)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (Conv2D)        (None, 120, 34, 60)  207480      block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 120, 34, 60)  480         block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (Activation)    (None, 120, 34, 60)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (Conv2D)        (None, 192, 34, 60)  207552      block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Conv2D) (None, 192, 34, 60)  37056       block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 192, 34, 60)  0           block_4a_bn_2[0][0]              \n",
      "                                                                 block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (Activation)      (None, 192, 34, 60)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (Conv2D)        (None, 56, 34, 60)   96824       block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 56, 34, 60)   224         block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (Activation)    (None, 56, 34, 60)   0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (Conv2D)        (None, 192, 34, 60)  96960       block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 192, 34, 60)  0           block_4b_bn_2[0][0]              \n",
      "                                                                 block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (Activation)      (None, 192, 34, 60)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_bbox (Conv2D)            (None, 4, 34, 60)    772         block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_cov (Conv2D)             (None, 1, 34, 60)    193         block_4b_relu[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,739,981\n",
      "Trainable params: 1,618,109\n",
      "Non-trainable params: 121,872\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "2022-12-23 02:57:16,199 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "2022-12-23 02:57:16,199 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "2022-12-23 02:57:16,199 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "2022-12-23 02:57:16,200 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "2022-12-23 02:57:16,200 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2022-12-23 02:57:16,573 [INFO] tensorflow: Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2022-12-23 02:57:16,945 [INFO] tensorflow: Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2022-12-23 02:57:17,276 [INFO] tensorflow: Done running local_init_op.\n",
      "2022-12-23 02:57:17,919 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 37, 0.00s/step\n",
      "2022-12-23 02:58:00,076 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 37, 4.22s/step\n",
      "2022-12-23 02:58:36,082 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 37, 3.60s/step\n",
      "2022-12-23 02:59:11,843 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 37, 3.58s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 308849/308849 [00:16<00:00, 18903.68it/s]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-12-23 02:59:57,785 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-12-23 02:59:57,786 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "\n",
      "Validation cost: 0.004298\n",
      "Mean average_precision (in %): 2.9080\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                              2.90803\n",
      "\n",
      "Median Inference Time: 0.017385\n",
      "2022-12-23 02:59:57,827 [INFO] __main__: Evaluation complete.\n",
      "Time taken to run __main__:main: 0:02:45.207400.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the pruned model\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector_pruned/resnet18_detector_pruned.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9212c-dd1d-4c3c-9200-4cd753c2e9fd",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Model Comparison ####\n",
    "**Instructions**: \n",
    "* Study the outputs regarding the size and mean average precision (mAP) of the unpruned and pruned model. \n",
    "* Note down how the two models compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23907aa6-ee89-4ca5-b6d5-c8f4eb73dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WRITE ANSWERS HERE #####\n",
    "#\n",
    "# \n",
    "#\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3cc1b-8ef0-4dce-8b6a-744bd21bf951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### WRITE ANSWERS HERE #####\n",
    "#\n",
    "# The pruned model is significantly smaller in size but has a lower mean average precision. \n",
    "#\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8377f0e-5c98-440a-92be-12fd0fd6578b",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cef0d-e283-47b5-8918-32e20098dc1c",
   "metadata": {},
   "source": [
    "<a name='s2.3'></a>\n",
    "### Retrain Pruned Model with Quantization-Aware Training ###\n",
    "To regain the accuracy, we recommend to retrain this pruned model over the same data set using the `train` subtask with an updated spec file that points to the newly pruned model as the pre-trained model file. There are several things to consider when retraining: \n",
    "* The `regularizer` option should be turned off in the `training_config` for DetectNet_v2 to recover the accuracy when retraining a pruned model. It can be done by setting the regularizer type to `NO_REG`. All other parameters may be retained in the spec file from the previous training.\n",
    "* The `load_graph` option should be set to `true` in the `model_config` to load the pruned model graph. \n",
    "* If after retraining, the model shows some decrease in mAP, it could be that the originally trained model was pruned a little too much. Please try reducing the pruning threshold (thereby reducing the pruning ratio) and use the new model to retrain.\n",
    "* _Optionally_, DetectNet_v2 supports **Quantization-Aware Training** to help with optmizing the model. \n",
    "\n",
    "Deep neural network (DNN) models, such as those routinely used video AI applications, are typically trained on servers with high-end GPUs available in data centers or private/public clouds. Such systems often use **floating-point 32-bit** arithmetic to take advantage of the wider dynamic range for the weights. After a model is trained, however, it often must be deployed at the edge on hardware that has less computational resources and power budget. Running a DNN inference using the full 32-bit representation is not practical for real-time analysis given the compute, memory, and power constraints of the edge. To help reduce the compute budget, while not compromising on the structure and number of parameters in the model, we can run inference at a lower precision. It is advantageous in many cases to use **8-bit integer numbers** for weights. The challenge is that simply rounding the weights after training may result in a lower accuracy model, especially if the weights have a wide dynamic range. While 8-bit **quantization** is appealing to save compute and memory budgets, it is a lossy process. During quantization, a small range of floating-point numbers are squeezed to a fixed number of information buckets. This results in loss of information. In another words, the minute differences which could originally be resolved using 32-bit representations are now lost because they get quantized to the same bucket in 8-bit representations. This is like the rounding errors that one encounters when representing fractional numbers as integers. To maintain accuracy during inferences at lower precision, it is important to try and mitigate errors arising due to this loss of information with Quantization-Aware Training. QAT is used to train DNNs for lower precision INT8 deployment without compromising on accuracy. It emulates the inference time quantization when training a model that may then be used by downstream inference platforms to generate actual quantized models. The error from quantization weights and tensors to INT8 is modeled during training, allowing the model to adapt and mitigate the error. Technically, during QAT the model constructed in the training graph is modified to: \n",
    "1. Replace existing notes with nodes that support fake quantization of its weights. \n",
    "2. Convert existing activation to ReLU-6 (except the output nodes). \n",
    "3. Add Quantize and De-Quantize (QDQ) nodes to compute the dynamic ranges of the intermediate tensors.  \n",
    "\n",
    "The dynamic ranges computed during training are serialized to a **cache file** that is used at inference. \n",
    "\n",
    "<p><img src='images/qat_training.png' width=720></p>\n",
    "\n",
    "More information about Quantization-Aware Training can be found [here](https://developer.nvidia.com/blog/improving-int8-accuracy-using-quantization-aware-training-and-tao-toolkit/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de5aed-4c9e-49fa-94ba-ad962ee578ef",
   "metadata": {},
   "source": [
    "<a name='e3'></a>\n",
    "#### Exercise #3 - Convert Pruned Model to QAT and Retrain ####\n",
    "Supported models can be converted to QAT models by setting the `enable_qat` parameter in the `training_config` component of the spec file to `true`. When creating a training configuration file for retraining, only the `enable_qat` and `regularizer` from the `training_config` component, and `pretrained_model_file` and `load_graph` from the `model_config` component are updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e36747-905d-4923-ba79-7e560459569e",
   "metadata": {},
   "source": [
    "**Instructions**:<br>\n",
    "* Modify the `model_config`[(separate qat version here)](spec_files/model_config_qat.txt) section and the `training_config`[(separate qat version here)](spec_files/training_config_qat.txt) of the training configuration file by changing the `<FIXME>`s into acceptable values. Please **save changes** to the files.\n",
    "* Execute the below cells to retrain the pruned model with QAT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb96b-741b-4a24-aabe-d5a3ade58e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/model_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ce272-7130-471d-b88a-d892d8cce51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the config file\n",
    "!cat $SPEC_FILES_DIR/training_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6b0da-f9f0-4c11-8506-251c04f77be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " model_config {\n",
    "   arch: \"resnet\"\n",
    "   pretrained_model_file: \"/dli/task/tao_project/models/resnet18_detector_pruned/resnet18_detector_pruned.tlt\"\n",
    "   load_graph: true\n",
    "   freeze_blocks: 0\n",
    "   freeze_blocks: 1\n",
    "   num_layers: 18\n",
    "   use_pooling: false\n",
    "   use_batch_norm: true\n",
    "   dropout_rate: 0.0\n",
    "   objective_set: {\n",
    "     cov: {}\n",
    "     bbox: {\n",
    "       scale: 35.0\n",
    "       offset: 0.5\n",
    "     }\n",
    "   }\n",
    " }\n",
    "\n",
    " training_config: {\n",
    "   batch_size_per_gpu: 16\n",
    "   num_epochs: 10\n",
    "   enable_qat: true\n",
    "   learning_rate: {\n",
    "     soft_start_annealing_schedule: {\n",
    "       min_learning_rate: 5e-6\n",
    "       max_learning_rate: 5e-4\n",
    "       soft_start: 0.1\n",
    "       annealing: 0.7\n",
    "     }\n",
    "   }\n",
    "   regularizer: {\n",
    "     type: L1\n",
    "     weight: 3e-9\n",
    "   }\n",
    "   optimizer: {\n",
    "     adam: {\n",
    "       epsilon: 1e-08\n",
    "       beta1: 0.9\n",
    "       beta2: 0.999\n",
    "     }\n",
    "   }\n",
    "   cost_scaling: {\n",
    "     enabled: false\n",
    "     initial_exponent: 20.0\n",
    "     increment: 0.005\n",
    "     decrement: 1.0\n",
    "   }\n",
    "   checkpoint_interval: 5\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55925a-cd4f-42da-a20d-59250b5efe06",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ff8a43-4108-44f8-8dc6-1be8c260c9c7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config: {\n",
      "  data_sources: {\n",
      "    tfrecords_path: \"/dli/task/tao_project/data/tfrecords/kitti_trainval/*\"\n",
      "    image_directory_path: \"/dli/task/tao_project/data/training\"\n",
      "  }\n",
      "  image_extension: \"png\"\n",
      "  target_class_mapping: {\n",
      "       key: \"car\"\n",
      "       value: \"car\"\n",
      "   }\n",
      "   validation_fold: 0\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      "augmentation_config: {\n",
      "   preprocessing: {\n",
      "     output_image_width: 960\n",
      "     output_image_height: 544\n",
      "     output_image_channel: 3\n",
      "     min_bbox_width: 1.0\n",
      "     min_bbox_height: 1.0\n",
      "   }\n",
      "   spatial_augmentation: {\n",
      "     hflip_probability: 0.5\n",
      "     vflip_probability: 0.5\n",
      "     zoom_min: 1.0\n",
      "     zoom_max: 1.0\n",
      "     translate_max_x: 8.0\n",
      "     translate_max_y: 8.0\n",
      "   }\n",
      "   color_augmentation: {\n",
      "     color_shift_stddev: 0.0\n",
      "     hue_rotation_max: 25.0\n",
      "    saturation_shift_max: 0.2\n",
      "    contrast_scale_max: 0.1\n",
      "     contrast_center: 0.5\n",
      "   }\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      " model_config {\n",
      "   arch: \"resnet\"\n",
      "   pretrained_model_file: \"/dli/task/tao_project/models/resnet18_detector_pruned/resnet18_detector_pruned.tlt\"\n",
      "   load_graph: true\n",
      "   freeze_blocks: 0\n",
      "   freeze_blocks: 1\n",
      "   num_layers: 18\n",
      "   use_pooling: false\n",
      "   use_batch_norm: true\n",
      "   dropout_rate: 0.0\n",
      "   objective_set: {\n",
      "     cov: {}\n",
      "     bbox: {\n",
      "       scale: 35.0\n",
      "       offset: 0.5\n",
      "     }\n",
      "   }\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      "bbox_rasterizer_config: {\n",
      "   target_class_config: {\n",
      "     key: \"car\"\n",
      "    value: {\n",
      "      cov_center_x: 0.5\n",
      "       cov_center_y: 0.5\n",
      "       cov_radius_x: 0.4\n",
      "       cov_radius_y: 0.4\n",
      "       bbox_min_radius: 1.0\n",
      "     }\n",
      "   }\n",
      "   deadzone_radius: 0.4\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      "postprocessing_config: {\n",
      "   target_class_config: {\n",
      "     key: \"car\"\n",
      "     value: {\n",
      "       clustering_config: {\n",
      "         coverage_threshold: 0.005\n",
      "         dbscan_eps: 0.15\n",
      "         dbscan_min_samples: 0.05\n",
      "         minimum_bounding_box_height: 20\n",
      "       }\n",
      "     }\n",
      "   }\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      " training_config: {\n",
      "   batch_size_per_gpu: 16\n",
      "   num_epochs: 10\n",
      "   enable_qat: true\n",
      "   learning_rate: {\n",
      "     soft_start_annealing_schedule: {\n",
      "       min_learning_rate: 5e-6\n",
      "       max_learning_rate: 5e-4\n",
      "       soft_start: 0.1\n",
      "       annealing: 0.7\n",
      "     }\n",
      "   }\n",
      "   regularizer: {\n",
      "     type: L1\n",
      "     weight: 3e-9\n",
      "   }\n",
      "   optimizer: {\n",
      "     adam: {\n",
      "       epsilon: 1e-08\n",
      "       beta1: 0.9\n",
      "       beta2: 0.999\n",
      "     }\n",
      "   }\n",
      "   cost_scaling: {\n",
      "     enabled: false\n",
      "     initial_exponent: 20.0\n",
      "     increment: 0.005\n",
      "     decrement: 1.0\n",
      "   }\n",
      "   checkpoint_interval: 5\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n",
      "cost_function_config: {\n",
      "  target_classes: {\n",
      "    name: \"car\"\n",
      "    class_weight: 1.0\n",
      "    coverage_foreground_weight: 0.05\n",
      "    objectives: {\n",
      "      name: \"cov\"\n",
      "      initial_weight: 1.0\n",
      "      weight_target: 1.0\n",
      "    }\n",
      "    objectives {\n",
      "      name: \"bbox\"\n",
      "      initial_weight: 10.0\n",
      "      weight_target: 10.0\n",
      "    }\n",
      "  }\n",
      "  enable_autoweighting: true\n",
      "  max_objective_weight: 0.9999\n",
      "  min_objective_weight: 0.0001\n",
      "}\n",
      "########## LEAVE NEW LINE BELOW\n",
      "evaluation_config: {\n",
      "   average_precision_mode: INTEGRATE\n",
      "   validation_period_during_training: 5\n",
      "   first_validation_epoch: 1\n",
      "   minimum_detection_ground_truth_overlap: {\n",
      "     key: \"car\"\n",
      "     value: 0.7\n",
      "   }\n",
      "   evaluation_box_config {\n",
      "     key: \"car\"\n",
      "     value: {\n",
      "       minimum_height: 4\n",
      "       maximum_height: 9999\n",
      "       minimum_width: 4\n",
      "       maximum_width: 9999\n",
      "     }\n",
      "   }\n",
      " }\n",
      "########## LEAVE NEW LINE BELOW\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# UPDATED enable_qat and regularizer from training_config\n",
    "# UPDATED pretrained_model_file and load_graph from model_config\n",
    "# Combining configuration components in separate files and writing into one\n",
    "!cat $SPEC_FILES_DIR/dataset_config.txt \\\n",
    "     $SPEC_FILES_DIR/augmentation_config.txt \\\n",
    "     $SPEC_FILES_DIR/model_config_qat.txt \\\n",
    "     $SPEC_FILES_DIR/bbox_rasterizer_config.txt \\\n",
    "     $SPEC_FILES_DIR/postprocessing_config.txt \\\n",
    "     $SPEC_FILES_DIR/training_config_qat.txt \\\n",
    "     $SPEC_FILES_DIR/cost_function_config.txt \\\n",
    "     $SPEC_FILES_DIR/evaluation_config.txt \\\n",
    "     > $SPEC_FILES_DIR/combined_training_config_qat.txt\n",
    "!cat $SPEC_FILES_DIR/combined_training_config_qat.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec10c586-3205-44f6-9ce3-d17363b36628",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/checkpoint_saver_hook.py:25: The name tf.train.CheckpointSaverHook is deprecated. Please use tf.estimator.CheckpointSaverHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:68: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:68: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-12-23 03:05:58,231 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-12-23 03:05:58,231 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-12-23 03:05:58,827 [INFO] __main__: Loading experiment spec at /dli/task/spec_files/combined_training_config_qat.txt.\n",
      "2022-12-23 03:05:58,829 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /dli/task/spec_files/combined_training_config_qat.txt\n",
      "2022-12-23 03:05:59,138 [INFO] __main__: Cannot iterate over exactly 2315 samples with a batch size of 16; each epoch will therefore take one extra step.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2022-12-23 03:05:59,139 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2022-12-23 03:05:59,140 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-12-23 03:05:59,143 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-12-23 03:05:59,270 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-12-23 03:05:59,278 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-12-23 03:05:59,300 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-12-23 03:05:59,957 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-12-23 03:05:59,957 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-12-23 03:06:00,105 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2022-12-23 03:06:21,070 [INFO] iva.detectnet_v2.objectives.bbox_objective: Default L1 loss function will be used.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 544, 960)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1_qdq (QDQ)               (None, 3, 544, 960)  1           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (QuantizedConv2D)         (None, 24, 272, 480) 3552        input_1_qdq[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 24, 272, 480) 96          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (ReLU)             (None, 24, 272, 480) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1_qdq (QDQ)          (None, 24, 272, 480) 1           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (QuantizedConv2 (None, 48, 136, 240) 10416       activation_1_qdq[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 48, 136, 240) 192         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (ReLU)          (None, 48, 136, 240) 0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1_qdq (QDQ)       (None, 48, 136, 240) 1           block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (QuantizedConv2 (None, 64, 136, 240) 27712       block_1a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Quantiz (None, 64, 136, 240) 1600        activation_1_qdq[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 136, 240) 256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2_qdq (QDQ)         (None, 64, 136, 240) 1           block_1a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut_qdq (QDQ)  (None, 64, 136, 240) 1           block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 136, 240) 0           block_1a_bn_2_qdq[0][0]          \n",
      "                                                                 block_1a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_1_qdq (QDQ)                 (None, 64, 136, 240) 1           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (ReLU)            (None, 64, 136, 240) 0           add_1_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_qdq (QDQ)         (None, 64, 136, 240) 1           block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (QuantizedConv2 (None, 64, 136, 240) 36928       block_1a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (ReLU)          (None, 64, 136, 240) 0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1_qdq (QDQ)       (None, 64, 136, 240) 1           block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (QuantizedConv2 (None, 64, 136, 240) 36928       block_1b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2_qdq (QDQ)         (None, 64, 136, 240) 1           block_1b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 136, 240) 0           block_1b_bn_2_qdq[0][0]          \n",
      "                                                                 block_1a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_2_qdq (QDQ)                 (None, 64, 136, 240) 1           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (ReLU)            (None, 64, 136, 240) 0           add_2_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_qdq (QDQ)         (None, 64, 136, 240) 1           block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (QuantizedConv2 (None, 80, 68, 120)  46160       block_1b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 80, 68, 120)  320         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (ReLU)          (None, 80, 68, 120)  0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1_qdq (QDQ)       (None, 80, 68, 120)  1           block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (QuantizedConv2 (None, 112, 68, 120) 80752       block_2a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Quantiz (None, 112, 68, 120) 7280        block_1b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 112, 68, 120) 448         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2_qdq (QDQ)         (None, 112, 68, 120) 1           block_2a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut_qdq (QDQ)  (None, 112, 68, 120) 1           block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 112, 68, 120) 0           block_2a_bn_2_qdq[0][0]          \n",
      "                                                                 block_2a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_3_qdq (QDQ)                 (None, 112, 68, 120) 1           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (ReLU)            (None, 112, 68, 120) 0           add_3_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_qdq (QDQ)         (None, 112, 68, 120) 1           block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (QuantizedConv2 (None, 96, 68, 120)  96864       block_2a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 96, 68, 120)  384         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (ReLU)          (None, 96, 68, 120)  0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1_qdq (QDQ)       (None, 96, 68, 120)  1           block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (QuantizedConv2 (None, 112, 68, 120) 96880       block_2b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2_qdq (QDQ)         (None, 112, 68, 120) 1           block_2b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 112, 68, 120) 0           block_2b_bn_2_qdq[0][0]          \n",
      "                                                                 block_2a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_4_qdq (QDQ)                 (None, 112, 68, 120) 1           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (ReLU)            (None, 112, 68, 120) 0           add_4_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_qdq (QDQ)         (None, 112, 68, 120) 1           block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (QuantizedConv2 (None, 104, 34, 60)  104936      block_2b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 104, 34, 60)  416         block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (ReLU)          (None, 104, 34, 60)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1_qdq (QDQ)       (None, 104, 34, 60)  1           block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (QuantizedConv2 (None, 192, 34, 60)  179904      block_3a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Quantiz (None, 192, 34, 60)  21696       block_2b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_3a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut_qdq (QDQ)  (None, 192, 34, 60)  1           block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 192, 34, 60)  0           block_3a_bn_2_qdq[0][0]          \n",
      "                                                                 block_3a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_5_qdq (QDQ)                 (None, 192, 34, 60)  1           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (ReLU)            (None, 192, 34, 60)  0           add_5_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (QuantizedConv2 (None, 96, 34, 60)   165984      block_3a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 96, 34, 60)   384         block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (ReLU)          (None, 96, 34, 60)   0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1_qdq (QDQ)       (None, 96, 34, 60)   1           block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (QuantizedConv2 (None, 192, 34, 60)  166080      block_3b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_3b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 192, 34, 60)  0           block_3b_bn_2_qdq[0][0]          \n",
      "                                                                 block_3a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_6_qdq (QDQ)                 (None, 192, 34, 60)  1           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (ReLU)            (None, 192, 34, 60)  0           add_6_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (QuantizedConv2 (None, 120, 34, 60)  207480      block_3b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 120, 34, 60)  480         block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (ReLU)          (None, 120, 34, 60)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1_qdq (QDQ)       (None, 120, 34, 60)  1           block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (QuantizedConv2 (None, 192, 34, 60)  207552      block_4a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Quantiz (None, 192, 34, 60)  37056       block_3b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_4a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut_qdq (QDQ)  (None, 192, 34, 60)  1           block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 192, 34, 60)  0           block_4a_bn_2_qdq[0][0]          \n",
      "                                                                 block_4a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_7_qdq (QDQ)                 (None, 192, 34, 60)  1           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (ReLU)            (None, 192, 34, 60)  0           add_7_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (QuantizedConv2 (None, 56, 34, 60)   96824       block_4a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 56, 34, 60)   224         block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (ReLU)          (None, 56, 34, 60)   0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1_qdq (QDQ)       (None, 56, 34, 60)   1           block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (QuantizedConv2 (None, 192, 34, 60)  96960       block_4b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_4b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 192, 34, 60)  0           block_4b_bn_2_qdq[0][0]          \n",
      "                                                                 block_4a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_8_qdq (QDQ)                 (None, 192, 34, 60)  1           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (ReLU)            (None, 192, 34, 60)  0           add_8_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_bbox (Conv2D)            (None, 4, 34, 60)    772         block_4b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_cov (Conv2D)             (None, 1, 34, 60)    193         block_4b_relu_qdq[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,740,019\n",
      "Trainable params: 1,618,109\n",
      "Non-trainable params: 121,910\n",
      "__________________________________________________________________________________________________\n",
      "2022-12-23 03:06:23,913 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-12-23 03:06:23,913 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-12-23 03:06:23,913 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-12-23 03:06:23,913 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: 4\n",
      "2022-12-23 03:06:23,913 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 2315, number of sources: 1, batch size per gpu: 16, steps: 145\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "2022-12-23 03:06:23,956 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523630>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:23,995 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523630>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523630>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:24,021 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-12-23 03:06:24,275 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: True - shard 0 of 1\n",
      "2022-12-23 03:06:24,281 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-12-23 03:06:24,281 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae8766f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae8766f28>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:24,296 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae8766f28>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae8766f28>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:24,669 [INFO] __main__: Found 2315 samples in training set\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "2022-12-23 03:06:24,775 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:89: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "2022-12-23 03:06:24,886 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:89: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "2022-12-23 03:06:24,902 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/training_proto_utilities.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-12-23 03:06:25,660 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-12-23 03:06:25,670 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/model/detectnet_model.py:587: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "2022-12-23 03:06:25,673 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/model/detectnet_model.py:587: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "2022-12-23 03:06:26,883 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-12-23 03:06:26,884 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-12-23 03:06:26,884 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-12-23 03:06:26,884 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: 4\n",
      "2022-12-23 03:06:26,884 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 578, number of sources: 1, batch size per gpu: 16, steps: 37\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523748>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:26,895 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7fdb78523748>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:27,043 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-12-23 03:06:27,286 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: False - shard 0 of 1\n",
      "2022-12-23 03:06:27,291 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-12-23 03:06:27,291 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae097d2e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae097d2e8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:27,305 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae097d2e8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7fdae097d2e8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:06:27,542 [INFO] __main__: Found 578 samples in validation set\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/validation_hook.py:40: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "2022-12-23 03:06:28,100 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/validation_hook.py:40: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "2022-12-23 03:06:29,644 [INFO] __main__: Checkpoint interval: 5\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:108: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "2022-12-23 03:06:29,645 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/scripts/train.py:108: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "2022-12-23 03:06:29,645 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "2022-12-23 03:06:29,645 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "2022-12-23 03:06:29,646 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:59: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
      "\n",
      "2022-12-23 03:06:29,648 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:59: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:60: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\n",
      "\n",
      "2022-12-23 03:06:29,648 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:60: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:73: The name tf.train.StepCounterHook is deprecated. Please use tf.estimator.StepCounterHook instead.\n",
      "\n",
      "2022-12-23 03:06:29,649 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:73: The name tf.train.StepCounterHook is deprecated. Please use tf.estimator.StepCounterHook instead.\n",
      "\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2022-12-23 03:06:29,649 [INFO] tensorflow: Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:99: The name tf.train.SummarySaverHook is deprecated. Please use tf.estimator.SummarySaverHook instead.\n",
      "\n",
      "2022-12-23 03:06:29,649 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/tfhooks/utils.py:99: The name tf.train.SummarySaverHook is deprecated. Please use tf.estimator.SummarySaverHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "2022-12-23 03:06:29,650 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2022-12-23 03:06:31,176 [INFO] tensorflow: Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2022-12-23 03:06:33,471 [INFO] tensorflow: Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2022-12-23 03:06:34,295 [INFO] tensorflow: Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for step-0.\n",
      "2022-12-23 03:06:43,199 [INFO] tensorflow: Saving checkpoints for step-0.\n",
      "INFO:tensorflow:epoch = 0.0, learning_rate = 4.9999994e-06, loss = 0.01521794, step = 0\n",
      "2022-12-23 03:07:33,243 [INFO] tensorflow: epoch = 0.0, learning_rate = 4.9999994e-06, loss = 0.01521794, step = 0\n",
      "2022-12-23 03:07:33,244 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 0/10: loss: 0.01522 learning rate: 0.00000 Time taken: 0:00:00 ETA: 0:00:00\n",
      "2022-12-23 03:07:33,244 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 1.491\n",
      "INFO:tensorflow:epoch = 0.041379310344827586, learning_rate = 6.0496245e-06, loss = 0.014600972, step = 6 (5.655 sec)\n",
      "2022-12-23 03:07:38,897 [INFO] tensorflow: epoch = 0.041379310344827586, learning_rate = 6.0496245e-06, loss = 0.014600972, step = 6 (5.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.35214\n",
      "2022-12-23 03:07:43,597 [INFO] tensorflow: global_step/sec: 1.35214\n",
      "INFO:tensorflow:epoch = 0.1103448275862069, learning_rate = 8.311121e-06, loss = 0.013931839, step = 16 (5.864 sec)\n",
      "2022-12-23 03:07:44,761 [INFO] tensorflow: epoch = 0.1103448275862069, learning_rate = 8.311121e-06, loss = 0.013931839, step = 16 (5.864 sec)\n",
      "2022-12-23 03:07:49,414 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 14.871\n",
      "INFO:tensorflow:epoch = 0.1793103448275862, learning_rate = 1.141802e-05, loss = 0.0132047795, step = 26 (5.793 sec)\n",
      "2022-12-23 03:07:50,554 [INFO] tensorflow: epoch = 0.1793103448275862, learning_rate = 1.141802e-05, loss = 0.0132047795, step = 26 (5.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.7224\n",
      "2022-12-23 03:07:51,726 [INFO] tensorflow: global_step/sec: 1.7224\n",
      "INFO:tensorflow:epoch = 0.2482758620689655, learning_rate = 1.568634e-05, loss = 0.011485213, step = 36 (5.804 sec)\n",
      "2022-12-23 03:07:56,358 [INFO] tensorflow: epoch = 0.2482758620689655, learning_rate = 1.568634e-05, loss = 0.011485213, step = 36 (5.804 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71127\n",
      "2022-12-23 03:07:59,907 [INFO] tensorflow: global_step/sec: 1.71127\n",
      "INFO:tensorflow:epoch = 0.3172413793103448, learning_rate = 2.1550277e-05, loss = 0.010584358, step = 46 (5.945 sec)\n",
      "2022-12-23 03:08:02,304 [INFO] tensorflow: epoch = 0.3172413793103448, learning_rate = 2.1550277e-05, loss = 0.010584358, step = 46 (5.945 sec)\n",
      "2022-12-23 03:08:04,094 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.248\n",
      "INFO:tensorflow:epoch = 0.38620689655172413, learning_rate = 2.9606264e-05, loss = 0.009365524, step = 56 (5.873 sec)\n",
      "2022-12-23 03:08:08,176 [INFO] tensorflow: epoch = 0.38620689655172413, learning_rate = 2.9606264e-05, loss = 0.009365524, step = 56 (5.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69273\n",
      "2022-12-23 03:08:08,177 [INFO] tensorflow: global_step/sec: 1.69273\n",
      "INFO:tensorflow:epoch = 0.45517241379310347, learning_rate = 4.0673807e-05, loss = 0.008472368, step = 66 (5.880 sec)\n",
      "2022-12-23 03:08:14,056 [INFO] tensorflow: epoch = 0.45517241379310347, learning_rate = 4.0673807e-05, loss = 0.008472368, step = 66 (5.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6301\n",
      "2022-12-23 03:08:16,766 [INFO] tensorflow: global_step/sec: 1.6301\n",
      "2022-12-23 03:08:19,552 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.877\n",
      "INFO:tensorflow:epoch = 0.5172413793103449, learning_rate = 5.4131837e-05, loss = 0.0081160795, step = 75 (6.123 sec)\n",
      "2022-12-23 03:08:20,179 [INFO] tensorflow: epoch = 0.5172413793103449, learning_rate = 5.4131837e-05, loss = 0.0081160795, step = 75 (6.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.608\n",
      "2022-12-23 03:08:25,472 [INFO] tensorflow: global_step/sec: 1.608\n",
      "INFO:tensorflow:epoch = 0.5862068965517241, learning_rate = 7.436756e-05, loss = 0.0074473615, step = 85 (5.855 sec)\n",
      "2022-12-23 03:08:26,033 [INFO] tensorflow: epoch = 0.5862068965517241, learning_rate = 7.436756e-05, loss = 0.0074473615, step = 85 (5.855 sec)\n",
      "INFO:tensorflow:epoch = 0.6551724137931034, learning_rate = 0.00010216797, loss = 0.0071189585, step = 95 (5.862 sec)\n",
      "2022-12-23 03:08:31,896 [INFO] tensorflow: epoch = 0.6551724137931034, learning_rate = 0.00010216797, loss = 0.0071189585, step = 95 (5.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70857\n",
      "2022-12-23 03:08:33,666 [INFO] tensorflow: global_step/sec: 1.70857\n",
      "2022-12-23 03:08:34,265 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.188\n",
      "INFO:tensorflow:epoch = 0.7241379310344828, learning_rate = 0.00014036085, loss = 0.0062834085, step = 105 (5.904 sec)\n",
      "2022-12-23 03:08:37,800 [INFO] tensorflow: epoch = 0.7241379310344828, learning_rate = 0.00014036085, loss = 0.0062834085, step = 105 (5.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68912\n",
      "2022-12-23 03:08:41,955 [INFO] tensorflow: global_step/sec: 1.68912\n",
      "INFO:tensorflow:epoch = 0.7931034482758621, learning_rate = 0.00019283114, loss = 0.0054073576, step = 115 (5.896 sec)\n",
      "2022-12-23 03:08:43,696 [INFO] tensorflow: epoch = 0.7931034482758621, learning_rate = 0.00019283114, loss = 0.0054073576, step = 115 (5.896 sec)\n",
      "2022-12-23 03:08:49,027 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.097\n",
      "INFO:tensorflow:epoch = 0.8620689655172413, learning_rate = 0.00026491587, loss = 0.0047749146, step = 125 (5.917 sec)\n",
      "2022-12-23 03:08:49,613 [INFO] tensorflow: epoch = 0.8620689655172413, learning_rate = 0.00026491587, loss = 0.0047749146, step = 125 (5.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69401\n",
      "2022-12-23 03:08:50,219 [INFO] tensorflow: global_step/sec: 1.69401\n",
      "INFO:tensorflow:epoch = 0.9310344827586207, learning_rate = 0.00036394785, loss = 0.002550272, step = 135 (5.888 sec)\n",
      "2022-12-23 03:08:55,501 [INFO] tensorflow: epoch = 0.9310344827586207, learning_rate = 0.00036394785, loss = 0.002550272, step = 135 (5.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70416\n",
      "2022-12-23 03:08:58,434 [INFO] tensorflow: global_step/sec: 1.70416\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Bootstrap : Using [0]lo:127.0.0.1<0> [1]eth0:172.18.0.3<0>\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO NET/IB : No device found.\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO NET/Socket : Using [0]lo:127.0.0.1<0> [1]eth0:172.18.0.3<0>\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Using network Socket\n",
      "NCCL version 2.7.8+cuda11.1\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 00/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 01/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 02/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 03/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 04/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 05/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 06/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 07/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 08/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 09/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 10/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 11/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 12/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 13/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 14/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 15/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 16/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 17/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 18/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 19/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 20/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 21/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 22/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 23/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 24/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 25/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 26/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 27/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 28/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 29/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 30/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Channel 31/32 :    0\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [1] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [2] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [3] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [4] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [5] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [6] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [7] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [8] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [9] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [10] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [11] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [12] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [13] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [14] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [15] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [16] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [17] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [18] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [19] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [20] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [21] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [22] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [23] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [24] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [25] -1/-1/-1->0->-1|-1->0->-1/-1/-1 [26] -1/-1/-1->0->-1|-1->0->\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\n",
      "09f5df2d607c:2154:2166 [0] NCCL INFO comm 0x7fdb18321410 rank 0 nranks 1 cudaDev 0 busId 1e0 - Init COMPLETE\n",
      "2022-12-23 03:09:01,242 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 36, 0.00s/step\n",
      "2022-12-23 03:09:10,210 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 36, 0.90s/step\n",
      "2022-12-23 03:09:16,518 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 36, 0.63s/step\n",
      "2022-12-23 03:09:23,455 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 36, 0.69s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 9032/9032 [00:00<00:00, 18651.67it/s]\n",
      "Epoch 1/10\n",
      "=========================\n",
      "\n",
      "Validation cost: 0.000434\n",
      "Mean average_precision (in %): 0.0187\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                            0.0187196\n",
      "\n",
      "Median Inference Time: 0.017750\n",
      "INFO:tensorflow:epoch = 1.0, learning_rate = 0.00049999997, loss = 0.00022689767, step = 145 (33.511 sec)\n",
      "2022-12-23 03:09:29,012 [INFO] tensorflow: epoch = 1.0, learning_rate = 0.00049999997, loss = 0.00022689767, step = 145 (33.511 sec)\n",
      "2022-12-23 03:09:29,013 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 1/10: loss: 0.00023 learning rate: 0.00050 Time taken: 0:02:05.927550 ETA: 0:18:53.347951\n",
      "2022-12-23 03:09:31,372 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 9.446\n",
      "INFO:tensorflow:global_step/sec: 0.390543\n",
      "2022-12-23 03:09:34,282 [INFO] tensorflow: global_step/sec: 0.390543\n",
      "INFO:tensorflow:epoch = 1.0689655172413792, learning_rate = 0.00049999997, loss = 0.00021139799, step = 155 (5.869 sec)\n",
      "2022-12-23 03:09:34,881 [INFO] tensorflow: epoch = 1.0689655172413792, learning_rate = 0.00049999997, loss = 0.00021139799, step = 155 (5.869 sec)\n",
      "INFO:tensorflow:epoch = 1.1379310344827587, learning_rate = 0.00049999997, loss = 0.00020184301, step = 165 (5.879 sec)\n",
      "2022-12-23 03:09:40,760 [INFO] tensorflow: epoch = 1.1379310344827587, learning_rate = 0.00049999997, loss = 0.00020184301, step = 165 (5.879 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68883\n",
      "2022-12-23 03:09:42,572 [INFO] tensorflow: global_step/sec: 1.68883\n",
      "2022-12-23 03:09:46,108 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.145\n",
      "INFO:tensorflow:epoch = 1.206896551724138, learning_rate = 0.00049999997, loss = 0.00021930685, step = 175 (5.927 sec)\n",
      "2022-12-23 03:09:46,687 [INFO] tensorflow: epoch = 1.206896551724138, learning_rate = 0.00049999997, loss = 0.00021930685, step = 175 (5.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69461\n",
      "2022-12-23 03:09:50,833 [INFO] tensorflow: global_step/sec: 1.69461\n",
      "INFO:tensorflow:epoch = 1.2758620689655171, learning_rate = 0.00049999997, loss = 0.00011385951, step = 185 (5.857 sec)\n",
      "2022-12-23 03:09:52,544 [INFO] tensorflow: epoch = 1.2758620689655171, learning_rate = 0.00049999997, loss = 0.00011385951, step = 185 (5.857 sec)\n",
      "INFO:tensorflow:epoch = 1.3448275862068966, learning_rate = 0.00049999997, loss = 0.00016250613, step = 195 (5.851 sec)\n",
      "2022-12-23 03:09:58,395 [INFO] tensorflow: epoch = 1.3448275862068966, learning_rate = 0.00049999997, loss = 0.00016250613, step = 195 (5.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71416\n",
      "2022-12-23 03:09:59,000 [INFO] tensorflow: global_step/sec: 1.71416\n",
      "2022-12-23 03:10:00,772 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.279\n",
      "INFO:tensorflow:epoch = 1.4137931034482758, learning_rate = 0.00049999997, loss = 0.00014866385, step = 205 (5.964 sec)\n",
      "2022-12-23 03:10:04,359 [INFO] tensorflow: epoch = 1.4137931034482758, learning_rate = 0.00049999997, loss = 0.00014866385, step = 205 (5.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69696\n",
      "2022-12-23 03:10:07,250 [INFO] tensorflow: global_step/sec: 1.69696\n",
      "INFO:tensorflow:epoch = 1.4827586206896552, learning_rate = 0.00049999997, loss = 0.00020555826, step = 215 (5.894 sec)\n",
      "2022-12-23 03:10:10,253 [INFO] tensorflow: epoch = 1.4827586206896552, learning_rate = 0.00049999997, loss = 0.00020555826, step = 215 (5.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67583\n",
      "2022-12-23 03:10:15,604 [INFO] tensorflow: global_step/sec: 1.67583\n",
      "2022-12-23 03:10:15,605 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.966\n",
      "INFO:tensorflow:epoch = 1.5517241379310345, learning_rate = 0.00049999997, loss = 0.00012374284, step = 225 (5.938 sec)\n",
      "2022-12-23 03:10:16,191 [INFO] tensorflow: epoch = 1.5517241379310345, learning_rate = 0.00049999997, loss = 0.00012374284, step = 225 (5.938 sec)\n",
      "INFO:tensorflow:epoch = 1.6206896551724137, learning_rate = 0.00049999997, loss = 0.000101641126, step = 235 (5.853 sec)\n",
      "2022-12-23 03:10:22,044 [INFO] tensorflow: epoch = 1.6206896551724137, learning_rate = 0.00049999997, loss = 0.000101641126, step = 235 (5.853 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70595\n",
      "2022-12-23 03:10:23,811 [INFO] tensorflow: global_step/sec: 1.70595\n",
      "INFO:tensorflow:epoch = 1.6896551724137931, learning_rate = 0.00049999997, loss = 0.00016007709, step = 245 (6.053 sec)\n",
      "2022-12-23 03:10:28,096 [INFO] tensorflow: epoch = 1.6896551724137931, learning_rate = 0.00049999997, loss = 0.00016007709, step = 245 (6.053 sec)\n",
      "2022-12-23 03:10:30,882 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.184\n",
      "INFO:tensorflow:global_step/sec: 1.54749\n",
      "2022-12-23 03:10:32,858 [INFO] tensorflow: global_step/sec: 1.54749\n",
      "INFO:tensorflow:epoch = 1.7517241379310344, learning_rate = 0.00049999997, loss = 9.214577e-05, step = 254 (5.952 sec)\n",
      "2022-12-23 03:10:34,049 [INFO] tensorflow: epoch = 1.7517241379310344, learning_rate = 0.00049999997, loss = 9.214577e-05, step = 254 (5.952 sec)\n",
      "INFO:tensorflow:epoch = 1.8206896551724139, learning_rate = 0.00049999997, loss = 0.00021919818, step = 264 (5.907 sec)\n",
      "2022-12-23 03:10:39,956 [INFO] tensorflow: epoch = 1.8206896551724139, learning_rate = 0.00049999997, loss = 0.00021919818, step = 264 (5.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68949\n",
      "2022-12-23 03:10:41,144 [INFO] tensorflow: global_step/sec: 1.68949\n",
      "INFO:tensorflow:epoch = 1.889655172413793, learning_rate = 0.00049999997, loss = 0.00019057539, step = 274 (5.889 sec)\n",
      "2022-12-23 03:10:45,845 [INFO] tensorflow: epoch = 1.889655172413793, learning_rate = 0.00049999997, loss = 0.00019057539, step = 274 (5.889 sec)\n",
      "2022-12-23 03:10:45,845 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.733\n",
      "INFO:tensorflow:global_step/sec: 1.6996\n",
      "2022-12-23 03:10:49,382 [INFO] tensorflow: global_step/sec: 1.6996\n",
      "INFO:tensorflow:epoch = 1.9586206896551723, learning_rate = 0.00049999997, loss = 0.0001772062, step = 284 (5.862 sec)\n",
      "2022-12-23 03:10:51,706 [INFO] tensorflow: epoch = 1.9586206896551723, learning_rate = 0.00049999997, loss = 0.0001772062, step = 284 (5.862 sec)\n",
      "2022-12-23 03:10:55,294 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 2/10: loss: 0.00010 learning rate: 0.00050 Time taken: 0:01:26.244638 ETA: 0:11:29.957108\n",
      "INFO:tensorflow:epoch = 2.027586206896552, learning_rate = 0.00049999997, loss = 0.00013275973, step = 294 (5.962 sec)\n",
      "2022-12-23 03:10:57,668 [INFO] tensorflow: epoch = 2.027586206896552, learning_rate = 0.00049999997, loss = 0.00013275973, step = 294 (5.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68937\n",
      "2022-12-23 03:10:57,669 [INFO] tensorflow: global_step/sec: 1.68937\n",
      "2022-12-23 03:11:00,592 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.125\n",
      "INFO:tensorflow:epoch = 2.096551724137931, learning_rate = 0.00049999997, loss = 0.00013843365, step = 304 (5.906 sec)\n",
      "2022-12-23 03:11:03,574 [INFO] tensorflow: epoch = 2.096551724137931, learning_rate = 0.00049999997, loss = 0.00013843365, step = 304 (5.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67644\n",
      "2022-12-23 03:11:06,020 [INFO] tensorflow: global_step/sec: 1.67644\n",
      "INFO:tensorflow:epoch = 2.1655172413793102, learning_rate = 0.00049999997, loss = 0.00015395986, step = 314 (5.986 sec)\n",
      "2022-12-23 03:11:09,560 [INFO] tensorflow: epoch = 2.1655172413793102, learning_rate = 0.00049999997, loss = 0.00015395986, step = 314 (5.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69357\n",
      "2022-12-23 03:11:14,286 [INFO] tensorflow: global_step/sec: 1.69357\n",
      "INFO:tensorflow:epoch = 2.2344827586206897, learning_rate = 0.00049999997, loss = 0.00013690507, step = 324 (5.943 sec)\n",
      "2022-12-23 03:11:15,502 [INFO] tensorflow: epoch = 2.2344827586206897, learning_rate = 0.00049999997, loss = 0.00013690507, step = 324 (5.943 sec)\n",
      "2022-12-23 03:11:15,502 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.827\n",
      "INFO:tensorflow:epoch = 2.303448275862069, learning_rate = 0.00049999997, loss = 0.000118303484, step = 334 (5.899 sec)\n",
      "2022-12-23 03:11:21,401 [INFO] tensorflow: epoch = 2.303448275862069, learning_rate = 0.00049999997, loss = 0.000118303484, step = 334 (5.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69319\n",
      "2022-12-23 03:11:22,555 [INFO] tensorflow: global_step/sec: 1.69319\n",
      "INFO:tensorflow:epoch = 2.372413793103448, learning_rate = 0.00049999997, loss = 0.00012427237, step = 344 (5.870 sec)\n",
      "2022-12-23 03:11:27,272 [INFO] tensorflow: epoch = 2.372413793103448, learning_rate = 0.00049999997, loss = 0.00012427237, step = 344 (5.870 sec)\n",
      "2022-12-23 03:11:30,285 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.059\n",
      "INFO:tensorflow:global_step/sec: 1.68413\n",
      "2022-12-23 03:11:30,868 [INFO] tensorflow: global_step/sec: 1.68413\n",
      "INFO:tensorflow:epoch = 2.4413793103448276, learning_rate = 0.00049999997, loss = 0.00016041151, step = 354 (5.954 sec)\n",
      "2022-12-23 03:11:33,226 [INFO] tensorflow: epoch = 2.4413793103448276, learning_rate = 0.00049999997, loss = 0.00016041151, step = 354 (5.954 sec)\n",
      "INFO:tensorflow:epoch = 2.503448275862069, learning_rate = 0.00049999997, loss = 0.000116142444, step = 363 (6.241 sec)\n",
      "2022-12-23 03:11:39,467 [INFO] tensorflow: epoch = 2.503448275862069, learning_rate = 0.00049999997, loss = 0.000116142444, step = 363 (6.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.52122\n",
      "2022-12-23 03:11:40,071 [INFO] tensorflow: global_step/sec: 1.52122\n",
      "INFO:tensorflow:epoch = 2.5724137931034483, learning_rate = 0.00049999997, loss = 0.00013420795, step = 373 (6.023 sec)\n",
      "2022-12-23 03:11:45,490 [INFO] tensorflow: epoch = 2.5724137931034483, learning_rate = 0.00049999997, loss = 0.00013420795, step = 373 (6.023 sec)\n",
      "2022-12-23 03:11:46,067 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.346\n",
      "INFO:tensorflow:global_step/sec: 1.67362\n",
      "2022-12-23 03:11:48,436 [INFO] tensorflow: global_step/sec: 1.67362\n",
      "INFO:tensorflow:epoch = 2.6413793103448278, learning_rate = 0.00049999997, loss = 0.00013268278, step = 383 (5.899 sec)\n",
      "2022-12-23 03:11:51,388 [INFO] tensorflow: epoch = 2.6413793103448278, learning_rate = 0.00049999997, loss = 0.00013268278, step = 383 (5.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70029\n",
      "2022-12-23 03:11:56,670 [INFO] tensorflow: global_step/sec: 1.70029\n",
      "INFO:tensorflow:epoch = 2.7103448275862068, learning_rate = 0.00049999997, loss = 0.00015910194, step = 393 (5.905 sec)\n",
      "2022-12-23 03:11:57,294 [INFO] tensorflow: epoch = 2.7103448275862068, learning_rate = 0.00049999997, loss = 0.00015910194, step = 393 (5.905 sec)\n",
      "2022-12-23 03:12:00,885 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.995\n",
      "INFO:tensorflow:epoch = 2.779310344827586, learning_rate = 0.00049999997, loss = 0.00011459261, step = 403 (5.937 sec)\n",
      "2022-12-23 03:12:03,231 [INFO] tensorflow: epoch = 2.779310344827586, learning_rate = 0.00049999997, loss = 0.00011459261, step = 403 (5.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67591\n",
      "2022-12-23 03:12:05,023 [INFO] tensorflow: global_step/sec: 1.67591\n",
      "INFO:tensorflow:epoch = 2.8482758620689657, learning_rate = 0.00049999997, loss = 8.114797e-05, step = 413 (6.009 sec)\n",
      "2022-12-23 03:12:09,240 [INFO] tensorflow: epoch = 2.8482758620689657, learning_rate = 0.00049999997, loss = 8.114797e-05, step = 413 (6.009 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66473\n",
      "2022-12-23 03:12:13,433 [INFO] tensorflow: global_step/sec: 1.66473\n",
      "INFO:tensorflow:epoch = 2.9172413793103447, learning_rate = 0.00049999997, loss = 0.0001866737, step = 423 (6.031 sec)\n",
      "2022-12-23 03:12:15,271 [INFO] tensorflow: epoch = 2.9172413793103447, learning_rate = 0.00049999997, loss = 0.0001866737, step = 423 (6.031 sec)\n",
      "2022-12-23 03:12:15,881 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.674\n",
      "INFO:tensorflow:epoch = 2.986206896551724, learning_rate = 0.00049999997, loss = 0.00010489162, step = 433 (5.962 sec)\n",
      "2022-12-23 03:12:21,233 [INFO] tensorflow: epoch = 2.986206896551724, learning_rate = 0.00049999997, loss = 0.00010489162, step = 433 (5.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67024\n",
      "2022-12-23 03:12:21,815 [INFO] tensorflow: global_step/sec: 1.67024\n",
      "2022-12-23 03:12:22,424 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 3/10: loss: 0.00014 learning rate: 0.00050 Time taken: 0:01:27.128906 ETA: 0:10:09.902339\n",
      "INFO:tensorflow:epoch = 3.0551724137931036, learning_rate = 0.00049999997, loss = 0.0001252827, step = 443 (5.993 sec)\n",
      "2022-12-23 03:12:27,227 [INFO] tensorflow: epoch = 3.0551724137931036, learning_rate = 0.00049999997, loss = 0.0001252827, step = 443 (5.993 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66199\n",
      "2022-12-23 03:12:30,239 [INFO] tensorflow: global_step/sec: 1.66199\n",
      "2022-12-23 03:12:30,810 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.793\n",
      "INFO:tensorflow:epoch = 3.1241379310344826, learning_rate = 0.00049999997, loss = 9.7468364e-05, step = 453 (5.940 sec)\n",
      "2022-12-23 03:12:33,166 [INFO] tensorflow: epoch = 3.1241379310344826, learning_rate = 0.00049999997, loss = 9.7468364e-05, step = 453 (5.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68185\n",
      "2022-12-23 03:12:38,563 [INFO] tensorflow: global_step/sec: 1.68185\n",
      "INFO:tensorflow:epoch = 3.193103448275862, learning_rate = 0.00049999997, loss = 0.00017562706, step = 463 (6.019 sec)\n",
      "2022-12-23 03:12:39,186 [INFO] tensorflow: epoch = 3.193103448275862, learning_rate = 0.00049999997, loss = 0.00017562706, step = 463 (6.019 sec)\n",
      "INFO:tensorflow:epoch = 3.2551724137931033, learning_rate = 0.00049999997, loss = 0.00010941146, step = 472 (5.755 sec)\n",
      "2022-12-23 03:12:44,941 [INFO] tensorflow: epoch = 3.2551724137931033, learning_rate = 0.00049999997, loss = 0.00010941146, step = 472 (5.755 sec)\n",
      "2022-12-23 03:12:46,357 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.730\n",
      "INFO:tensorflow:global_step/sec: 1.52756\n",
      "2022-12-23 03:12:47,728 [INFO] tensorflow: global_step/sec: 1.52756\n",
      "INFO:tensorflow:epoch = 3.3172413793103446, learning_rate = 0.00049999997, loss = 0.00011805481, step = 481 (5.786 sec)\n",
      "2022-12-23 03:12:50,728 [INFO] tensorflow: epoch = 3.3172413793103446, learning_rate = 0.00049999997, loss = 0.00011805481, step = 481 (5.786 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68945\n",
      "2022-12-23 03:12:56,015 [INFO] tensorflow: global_step/sec: 1.68945\n",
      "INFO:tensorflow:epoch = 3.386206896551724, learning_rate = 0.00049999997, loss = 0.00013859963, step = 491 (5.878 sec)\n",
      "2022-12-23 03:12:56,605 [INFO] tensorflow: epoch = 3.386206896551724, learning_rate = 0.00049999997, loss = 0.00013859963, step = 491 (5.878 sec)\n",
      "2022-12-23 03:13:01,180 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.985\n",
      "INFO:tensorflow:epoch = 3.4551724137931035, learning_rate = 0.00049999997, loss = 0.00015696953, step = 501 (5.732 sec)\n",
      "2022-12-23 03:13:02,337 [INFO] tensorflow: epoch = 3.4551724137931035, learning_rate = 0.00049999997, loss = 0.00015696953, step = 501 (5.732 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.73018\n",
      "2022-12-23 03:13:04,106 [INFO] tensorflow: global_step/sec: 1.73018\n",
      "INFO:tensorflow:epoch = 3.524137931034483, learning_rate = 0.00049999997, loss = 0.000120665725, step = 511 (5.910 sec)\n",
      "2022-12-23 03:13:08,247 [INFO] tensorflow: epoch = 3.524137931034483, learning_rate = 0.00049999997, loss = 0.000120665725, step = 511 (5.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68646\n",
      "2022-12-23 03:13:12,408 [INFO] tensorflow: global_step/sec: 1.68646\n",
      "INFO:tensorflow:epoch = 3.593103448275862, learning_rate = 0.00049999997, loss = 0.00013378957, step = 521 (5.941 sec)\n",
      "2022-12-23 03:13:14,188 [INFO] tensorflow: epoch = 3.593103448275862, learning_rate = 0.00049999997, loss = 0.00013378957, step = 521 (5.941 sec)\n",
      "2022-12-23 03:13:15,939 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.104\n",
      "INFO:tensorflow:epoch = 3.6620689655172414, learning_rate = 0.00049999997, loss = 0.00015360316, step = 531 (5.850 sec)\n",
      "2022-12-23 03:13:20,037 [INFO] tensorflow: epoch = 3.6620689655172414, learning_rate = 0.00049999997, loss = 0.00015360316, step = 531 (5.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70873\n",
      "2022-12-23 03:13:20,601 [INFO] tensorflow: global_step/sec: 1.70873\n",
      "INFO:tensorflow:epoch = 3.731034482758621, learning_rate = 0.00049999997, loss = 0.000107167994, step = 541 (5.749 sec)\n",
      "2022-12-23 03:13:25,786 [INFO] tensorflow: epoch = 3.731034482758621, learning_rate = 0.00049999997, loss = 0.000107167994, step = 541 (5.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72543\n",
      "2022-12-23 03:13:28,715 [INFO] tensorflow: global_step/sec: 1.72543\n",
      "2022-12-23 03:13:30,497 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.477\n",
      "INFO:tensorflow:epoch = 3.8, learning_rate = 0.00049999997, loss = 0.0001311256, step = 551 (5.829 sec)\n",
      "2022-12-23 03:13:31,615 [INFO] tensorflow: epoch = 3.8, learning_rate = 0.00049999997, loss = 0.0001311256, step = 551 (5.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70965\n",
      "2022-12-23 03:13:36,904 [INFO] tensorflow: global_step/sec: 1.70965\n",
      "INFO:tensorflow:epoch = 3.8689655172413793, learning_rate = 0.00049999997, loss = 0.00015089565, step = 561 (5.884 sec)\n",
      "2022-12-23 03:13:37,499 [INFO] tensorflow: epoch = 3.8689655172413793, learning_rate = 0.00049999997, loss = 0.00015089565, step = 561 (5.884 sec)\n",
      "INFO:tensorflow:epoch = 3.9379310344827587, learning_rate = 0.00049999997, loss = 0.00011701288, step = 571 (5.794 sec)\n",
      "2022-12-23 03:13:43,293 [INFO] tensorflow: epoch = 3.9379310344827587, learning_rate = 0.00049999997, loss = 0.00011701288, step = 571 (5.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72126\n",
      "2022-12-23 03:13:45,037 [INFO] tensorflow: global_step/sec: 1.72126\n",
      "2022-12-23 03:13:45,038 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.509\n",
      "2022-12-23 03:13:48,553 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 4/10: loss: 0.00014 learning rate: 0.00050 Time taken: 0:01:26.157572 ETA: 0:08:36.945429\n",
      "INFO:tensorflow:epoch = 4.006896551724138, learning_rate = 0.00049999997, loss = 9.622347e-05, step = 581 (5.861 sec)\n",
      "2022-12-23 03:13:49,154 [INFO] tensorflow: epoch = 4.006896551724138, learning_rate = 0.00049999997, loss = 9.622347e-05, step = 581 (5.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.58366\n",
      "2022-12-23 03:13:53,878 [INFO] tensorflow: global_step/sec: 1.58366\n",
      "INFO:tensorflow:epoch = 4.068965517241379, learning_rate = 0.00049999997, loss = 0.00014965379, step = 590 (5.950 sec)\n",
      "2022-12-23 03:13:55,104 [INFO] tensorflow: epoch = 4.068965517241379, learning_rate = 0.00049999997, loss = 0.00014965379, step = 590 (5.950 sec)\n",
      "2022-12-23 03:14:00,354 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.118\n",
      "INFO:tensorflow:epoch = 4.137931034482759, learning_rate = 0.00049999997, loss = 0.00012853947, step = 600 (5.841 sec)\n",
      "2022-12-23 03:14:00,945 [INFO] tensorflow: epoch = 4.137931034482759, learning_rate = 0.00049999997, loss = 0.00012853947, step = 600 (5.841 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69676\n",
      "2022-12-23 03:14:02,129 [INFO] tensorflow: global_step/sec: 1.69676\n",
      "INFO:tensorflow:epoch = 4.206896551724138, learning_rate = 0.00049999997, loss = 0.0001514067, step = 610 (5.881 sec)\n",
      "2022-12-23 03:14:06,826 [INFO] tensorflow: epoch = 4.206896551724138, learning_rate = 0.00049999997, loss = 0.0001514067, step = 610 (5.881 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70819\n",
      "2022-12-23 03:14:10,324 [INFO] tensorflow: global_step/sec: 1.70819\n",
      "INFO:tensorflow:epoch = 4.275862068965517, learning_rate = 0.00049999997, loss = 0.00013227918, step = 620 (5.816 sec)\n",
      "2022-12-23 03:14:12,642 [INFO] tensorflow: epoch = 4.275862068965517, learning_rate = 0.00049999997, loss = 0.00013227918, step = 620 (5.816 sec)\n",
      "2022-12-23 03:14:14,985 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.339\n",
      "INFO:tensorflow:epoch = 4.344827586206897, learning_rate = 0.00049999997, loss = 0.000115077026, step = 630 (5.827 sec)\n",
      "2022-12-23 03:14:18,469 [INFO] tensorflow: epoch = 4.344827586206897, learning_rate = 0.00049999997, loss = 0.000115077026, step = 630 (5.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.7187\n",
      "2022-12-23 03:14:18,470 [INFO] tensorflow: global_step/sec: 1.7187\n",
      "INFO:tensorflow:epoch = 4.413793103448276, learning_rate = 0.00049999997, loss = 0.00014185635, step = 640 (5.839 sec)\n",
      "2022-12-23 03:14:24,309 [INFO] tensorflow: epoch = 4.413793103448276, learning_rate = 0.00049999997, loss = 0.00014185635, step = 640 (5.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71156\n",
      "2022-12-23 03:14:26,650 [INFO] tensorflow: global_step/sec: 1.71156\n",
      "2022-12-23 03:14:29,561 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.443\n",
      "INFO:tensorflow:epoch = 4.482758620689655, learning_rate = 0.00049999997, loss = 0.000142035, step = 650 (5.844 sec)\n",
      "2022-12-23 03:14:30,153 [INFO] tensorflow: epoch = 4.482758620689655, learning_rate = 0.00049999997, loss = 0.000142035, step = 650 (5.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72305\n",
      "2022-12-23 03:14:34,775 [INFO] tensorflow: global_step/sec: 1.72305\n",
      "INFO:tensorflow:epoch = 4.551724137931035, learning_rate = 0.00049999997, loss = 0.00016479845, step = 660 (5.775 sec)\n",
      "2022-12-23 03:14:35,928 [INFO] tensorflow: epoch = 4.551724137931035, learning_rate = 0.00049999997, loss = 0.00016479845, step = 660 (5.775 sec)\n",
      "INFO:tensorflow:epoch = 4.620689655172414, learning_rate = 0.00049999997, loss = 0.00013555372, step = 670 (5.815 sec)\n",
      "2022-12-23 03:14:41,744 [INFO] tensorflow: epoch = 4.620689655172414, learning_rate = 0.00049999997, loss = 0.00013555372, step = 670 (5.815 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72656\n",
      "2022-12-23 03:14:42,883 [INFO] tensorflow: global_step/sec: 1.72656\n",
      "2022-12-23 03:14:44,064 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.582\n",
      "INFO:tensorflow:epoch = 4.689655172413793, learning_rate = 0.00049999997, loss = 0.00011967867, step = 680 (5.868 sec)\n",
      "2022-12-23 03:14:47,612 [INFO] tensorflow: epoch = 4.689655172413793, learning_rate = 0.00049999997, loss = 0.00011967867, step = 680 (5.868 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70661\n",
      "2022-12-23 03:14:51,087 [INFO] tensorflow: global_step/sec: 1.70661\n",
      "INFO:tensorflow:epoch = 4.758620689655173, learning_rate = 0.00049999997, loss = 7.372783e-05, step = 690 (5.801 sec)\n",
      "2022-12-23 03:14:53,413 [INFO] tensorflow: epoch = 4.758620689655173, learning_rate = 0.00049999997, loss = 7.372783e-05, step = 690 (5.801 sec)\n",
      "INFO:tensorflow:epoch = 4.820689655172414, learning_rate = 0.00049999997, loss = 0.00013411313, step = 699 (5.799 sec)\n",
      "2022-12-23 03:14:59,213 [INFO] tensorflow: epoch = 4.820689655172414, learning_rate = 0.00049999997, loss = 0.00013411313, step = 699 (5.799 sec)\n",
      "2022-12-23 03:14:59,213 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.405\n",
      "INFO:tensorflow:global_step/sec: 1.59066\n",
      "2022-12-23 03:14:59,888 [INFO] tensorflow: global_step/sec: 1.59066\n",
      "INFO:tensorflow:epoch = 4.889655172413793, learning_rate = 0.00049999997, loss = 9.703013e-05, step = 709 (6.091 sec)\n",
      "2022-12-23 03:15:05,303 [INFO] tensorflow: epoch = 4.889655172413793, learning_rate = 0.00049999997, loss = 9.703013e-05, step = 709 (6.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68924\n",
      "2022-12-23 03:15:08,176 [INFO] tensorflow: global_step/sec: 1.68924\n",
      "INFO:tensorflow:epoch = 4.958620689655173, learning_rate = 0.00049999997, loss = 0.00019592678, step = 719 (5.719 sec)\n",
      "2022-12-23 03:15:11,022 [INFO] tensorflow: epoch = 4.958620689655173, learning_rate = 0.00049999997, loss = 0.00019592678, step = 719 (5.719 sec)\n",
      "INFO:tensorflow:Saving checkpoints for step-725.\n",
      "2022-12-23 03:15:13,927 [INFO] tensorflow: Saving checkpoints for step-725.\n",
      "2022-12-23 03:15:15,607 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 24.400\n",
      "2022-12-23 03:15:16,210 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 5/10: loss: 0.00011 learning rate: 0.00050 Time taken: 0:01:27.630164 ETA: 0:07:18.150820\n",
      "INFO:tensorflow:epoch = 5.006896551724138, learning_rate = 0.00049999997, loss = 0.000101041456, step = 726 (5.765 sec)\n",
      "2022-12-23 03:15:16,787 [INFO] tensorflow: epoch = 5.006896551724138, learning_rate = 0.00049999997, loss = 0.000101041456, step = 726 (5.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.4321\n",
      "2022-12-23 03:15:17,952 [INFO] tensorflow: global_step/sec: 1.4321\n",
      "INFO:tensorflow:epoch = 5.075862068965517, learning_rate = 0.00049999997, loss = 0.00011067839, step = 736 (5.844 sec)\n",
      "2022-12-23 03:15:22,630 [INFO] tensorflow: epoch = 5.075862068965517, learning_rate = 0.00049999997, loss = 0.00011067839, step = 736 (5.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.7164\n",
      "2022-12-23 03:15:26,108 [INFO] tensorflow: global_step/sec: 1.7164\n",
      "INFO:tensorflow:epoch = 5.144827586206897, learning_rate = 0.00049999997, loss = 0.00012753971, step = 746 (5.805 sec)\n",
      "2022-12-23 03:15:28,435 [INFO] tensorflow: epoch = 5.144827586206897, learning_rate = 0.00049999997, loss = 0.00012753971, step = 746 (5.805 sec)\n",
      "2022-12-23 03:15:30,189 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.433\n",
      "INFO:tensorflow:epoch = 5.213793103448276, learning_rate = 0.00049999997, loss = 9.358559e-05, step = 756 (5.847 sec)\n",
      "2022-12-23 03:15:34,282 [INFO] tensorflow: epoch = 5.213793103448276, learning_rate = 0.00049999997, loss = 9.358559e-05, step = 756 (5.847 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71264\n",
      "2022-12-23 03:15:34,283 [INFO] tensorflow: global_step/sec: 1.71264\n",
      "INFO:tensorflow:epoch = 5.2827586206896555, learning_rate = 0.00049999997, loss = 0.00012666201, step = 766 (5.869 sec)\n",
      "2022-12-23 03:15:40,151 [INFO] tensorflow: epoch = 5.2827586206896555, learning_rate = 0.00049999997, loss = 0.00012666201, step = 766 (5.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70889\n",
      "2022-12-23 03:15:42,475 [INFO] tensorflow: global_step/sec: 1.70889\n",
      "2022-12-23 03:15:44,800 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.376\n",
      "INFO:tensorflow:epoch = 5.3517241379310345, learning_rate = 0.00049999997, loss = 0.0001656669, step = 776 (5.806 sec)\n",
      "2022-12-23 03:15:45,957 [INFO] tensorflow: epoch = 5.3517241379310345, learning_rate = 0.00049999997, loss = 0.0001656669, step = 776 (5.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72072\n",
      "2022-12-23 03:15:50,612 [INFO] tensorflow: global_step/sec: 1.72072\n",
      "INFO:tensorflow:epoch = 5.4206896551724135, learning_rate = 0.00049999997, loss = 0.00010685, step = 786 (5.834 sec)\n",
      "2022-12-23 03:15:51,791 [INFO] tensorflow: epoch = 5.4206896551724135, learning_rate = 0.00049999997, loss = 0.00010685, step = 786 (5.834 sec)\n",
      "INFO:tensorflow:epoch = 5.489655172413793, learning_rate = 0.00049999997, loss = 0.00010860635, step = 796 (5.829 sec)\n",
      "2022-12-23 03:15:57,620 [INFO] tensorflow: epoch = 5.489655172413793, learning_rate = 0.00049999997, loss = 0.00010860635, step = 796 (5.829 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71123\n",
      "2022-12-23 03:15:58,793 [INFO] tensorflow: global_step/sec: 1.71123\n",
      "2022-12-23 03:15:59,394 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.410\n",
      "INFO:tensorflow:epoch = 5.558620689655172, learning_rate = 0.00049999997, loss = 0.00015058645, step = 806 (6.302 sec)\n",
      "2022-12-23 03:16:03,922 [INFO] tensorflow: epoch = 5.558620689655172, learning_rate = 0.00049999997, loss = 0.00015058645, step = 806 (6.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.54003\n",
      "2022-12-23 03:16:07,884 [INFO] tensorflow: global_step/sec: 1.54003\n",
      "INFO:tensorflow:epoch = 5.620689655172414, learning_rate = 0.00049999997, loss = 8.999159e-05, step = 815 (5.729 sec)\n",
      "2022-12-23 03:16:09,651 [INFO] tensorflow: epoch = 5.620689655172414, learning_rate = 0.00049999997, loss = 8.999159e-05, step = 815 (5.729 sec)\n",
      "2022-12-23 03:16:14,894 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.806\n",
      "INFO:tensorflow:epoch = 5.689655172413793, learning_rate = 0.00049999997, loss = 0.00013128985, step = 825 (5.845 sec)\n",
      "2022-12-23 03:16:15,496 [INFO] tensorflow: epoch = 5.689655172413793, learning_rate = 0.00049999997, loss = 0.00013128985, step = 825 (5.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70893\n",
      "2022-12-23 03:16:16,076 [INFO] tensorflow: global_step/sec: 1.70893\n",
      "INFO:tensorflow:epoch = 5.758620689655173, learning_rate = 0.00049999997, loss = 0.00010198938, step = 835 (5.846 sec)\n",
      "2022-12-23 03:16:21,342 [INFO] tensorflow: epoch = 5.758620689655173, learning_rate = 0.00049999997, loss = 0.00010198938, step = 835 (5.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70315\n",
      "2022-12-23 03:16:24,296 [INFO] tensorflow: global_step/sec: 1.70315\n",
      "INFO:tensorflow:epoch = 5.827586206896552, learning_rate = 0.00049999997, loss = 8.247566e-05, step = 845 (5.895 sec)\n",
      "2022-12-23 03:16:27,237 [INFO] tensorflow: epoch = 5.827586206896552, learning_rate = 0.00049999997, loss = 8.247566e-05, step = 845 (5.895 sec)\n",
      "2022-12-23 03:16:29,560 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.276\n",
      "INFO:tensorflow:global_step/sec: 1.69795\n",
      "2022-12-23 03:16:32,541 [INFO] tensorflow: global_step/sec: 1.69795\n",
      "INFO:tensorflow:epoch = 5.896551724137931, learning_rate = 0.00049999997, loss = 0.00015694593, step = 855 (5.892 sec)\n",
      "2022-12-23 03:16:33,129 [INFO] tensorflow: epoch = 5.896551724137931, learning_rate = 0.00049999997, loss = 0.00015694593, step = 855 (5.892 sec)\n",
      "INFO:tensorflow:epoch = 5.9655172413793105, learning_rate = 0.00049999997, loss = 0.00013179691, step = 865 (5.937 sec)\n",
      "2022-12-23 03:16:39,066 [INFO] tensorflow: epoch = 5.9655172413793105, learning_rate = 0.00049999997, loss = 0.00013179691, step = 865 (5.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68417\n",
      "2022-12-23 03:16:40,854 [INFO] tensorflow: global_step/sec: 1.68417\n",
      "2022-12-23 03:16:41,421 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 36, 0.00s/step\n",
      "2022-12-23 03:17:13,154 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 36, 3.17s/step\n",
      "2022-12-23 03:17:44,972 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 36, 3.18s/step\n",
      "2022-12-23 03:18:16,696 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 36, 3.17s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 13196/13196 [00:00<00:00, 18285.19it/s]\n",
      "Epoch 6/10\n",
      "=========================\n",
      "\n",
      "Validation cost: 0.000087\n",
      "Mean average_precision (in %): 41.4949\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                              41.4949\n",
      "\n",
      "Median Inference Time: 0.018860\n",
      "INFO:tensorflow:epoch = 6.0, learning_rate = 0.00049999997, loss = 0.00012291672, step = 870 (118.588 sec)\n",
      "2022-12-23 03:18:37,653 [INFO] tensorflow: epoch = 6.0, learning_rate = 0.00049999997, loss = 0.00012291672, step = 870 (118.588 sec)\n",
      "2022-12-23 03:18:37,654 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 6/10: loss: 0.00012 learning rate: 0.00050 Time taken: 0:03:21.411897 ETA: 0:13:25.647588\n",
      "2022-12-23 03:18:39,960 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 3.067\n",
      "INFO:tensorflow:epoch = 6.068965517241379, learning_rate = 0.00049999997, loss = 0.00014190821, step = 880 (5.870 sec)\n",
      "2022-12-23 03:18:43,524 [INFO] tensorflow: epoch = 6.068965517241379, learning_rate = 0.00049999997, loss = 0.00014190821, step = 880 (5.870 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.11306\n",
      "2022-12-23 03:18:44,682 [INFO] tensorflow: global_step/sec: 0.11306\n",
      "INFO:tensorflow:epoch = 6.137931034482759, learning_rate = 0.00049999997, loss = 0.00012936565, step = 890 (5.846 sec)\n",
      "2022-12-23 03:18:49,370 [INFO] tensorflow: epoch = 6.137931034482759, learning_rate = 0.00049999997, loss = 0.00012936565, step = 890 (5.846 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69418\n",
      "2022-12-23 03:18:52,946 [INFO] tensorflow: global_step/sec: 1.69418\n",
      "2022-12-23 03:18:54,668 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.197\n",
      "INFO:tensorflow:epoch = 6.206896551724138, learning_rate = 0.00049999997, loss = 0.0001344774, step = 900 (5.899 sec)\n",
      "2022-12-23 03:18:55,268 [INFO] tensorflow: epoch = 6.206896551724138, learning_rate = 0.00049999997, loss = 0.0001344774, step = 900 (5.899 sec)\n",
      "INFO:tensorflow:epoch = 6.275862068965517, learning_rate = 0.00049999997, loss = 8.454933e-05, step = 910 (5.904 sec)\n",
      "2022-12-23 03:19:01,172 [INFO] tensorflow: epoch = 6.275862068965517, learning_rate = 0.00049999997, loss = 8.454933e-05, step = 910 (5.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70168\n",
      "2022-12-23 03:19:01,173 [INFO] tensorflow: global_step/sec: 1.70168\n",
      "INFO:tensorflow:epoch = 6.344827586206897, learning_rate = 0.00049999997, loss = 0.00014043196, step = 920 (5.953 sec)\n",
      "2022-12-23 03:19:07,126 [INFO] tensorflow: epoch = 6.344827586206897, learning_rate = 0.00049999997, loss = 0.00014043196, step = 920 (5.953 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68895\n",
      "2022-12-23 03:19:09,462 [INFO] tensorflow: global_step/sec: 1.68895\n",
      "2022-12-23 03:19:09,463 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.036\n",
      "INFO:tensorflow:epoch = 6.413793103448276, learning_rate = 0.00049999997, loss = 6.6237626e-05, step = 930 (5.872 sec)\n",
      "2022-12-23 03:19:12,998 [INFO] tensorflow: epoch = 6.413793103448276, learning_rate = 0.00049999997, loss = 6.6237626e-05, step = 930 (5.872 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68947\n",
      "2022-12-23 03:19:17,749 [INFO] tensorflow: global_step/sec: 1.68947\n",
      "INFO:tensorflow:epoch = 6.482758620689655, learning_rate = 0.00049999997, loss = 9.171452e-05, step = 940 (5.948 sec)\n",
      "2022-12-23 03:19:18,945 [INFO] tensorflow: epoch = 6.482758620689655, learning_rate = 0.00049999997, loss = 9.171452e-05, step = 940 (5.948 sec)\n",
      "INFO:tensorflow:epoch = 6.544827586206896, learning_rate = 0.00049999997, loss = 0.000114933835, step = 949 (5.747 sec)\n",
      "2022-12-23 03:19:24,692 [INFO] tensorflow: epoch = 6.544827586206896, learning_rate = 0.00049999997, loss = 0.000114933835, step = 949 (5.747 sec)\n",
      "2022-12-23 03:19:24,692 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.266\n",
      "INFO:tensorflow:global_step/sec: 1.56201\n",
      "2022-12-23 03:19:26,712 [INFO] tensorflow: global_step/sec: 1.56201\n",
      "INFO:tensorflow:epoch = 6.606896551724138, learning_rate = 0.00049999997, loss = 0.00010127828, step = 958 (5.622 sec)\n",
      "2022-12-23 03:19:30,314 [INFO] tensorflow: epoch = 6.606896551724138, learning_rate = 0.00049999997, loss = 0.00010127828, step = 958 (5.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68212\n",
      "2022-12-23 03:19:35,035 [INFO] tensorflow: global_step/sec: 1.68212\n",
      "INFO:tensorflow:epoch = 6.675862068965517, learning_rate = 0.00049999997, loss = 0.000104447376, step = 968 (5.884 sec)\n",
      "2022-12-23 03:19:36,198 [INFO] tensorflow: epoch = 6.675862068965517, learning_rate = 0.00049999997, loss = 0.000104447376, step = 968 (5.884 sec)\n",
      "2022-12-23 03:19:39,768 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.534\n",
      "INFO:tensorflow:epoch = 6.744827586206896, learning_rate = 0.00049999997, loss = 0.000117465024, step = 978 (5.896 sec)\n",
      "2022-12-23 03:19:42,094 [INFO] tensorflow: epoch = 6.744827586206896, learning_rate = 0.00049999997, loss = 0.000117465024, step = 978 (5.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69001\n",
      "2022-12-23 03:19:43,319 [INFO] tensorflow: global_step/sec: 1.69001\n",
      "INFO:tensorflow:epoch = 6.813793103448276, learning_rate = 0.00049999997, loss = 0.00014888187, step = 988 (5.980 sec)\n",
      "2022-12-23 03:19:48,074 [INFO] tensorflow: epoch = 6.813793103448276, learning_rate = 0.00049999997, loss = 0.00014888187, step = 988 (5.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67991\n",
      "2022-12-23 03:19:51,652 [INFO] tensorflow: global_step/sec: 1.67991\n",
      "INFO:tensorflow:epoch = 6.882758620689655, learning_rate = 0.00049999997, loss = 0.00017901024, step = 998 (5.912 sec)\n",
      "2022-12-23 03:19:53,986 [INFO] tensorflow: epoch = 6.882758620689655, learning_rate = 0.00049999997, loss = 0.00017901024, step = 998 (5.912 sec)\n",
      "2022-12-23 03:19:54,590 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.988\n",
      "INFO:tensorflow:epoch = 6.951724137931034, learning_rate = 0.00049999997, loss = 7.591517e-05, step = 1008 (5.905 sec)\n",
      "2022-12-23 03:19:59,891 [INFO] tensorflow: epoch = 6.951724137931034, learning_rate = 0.00049999997, loss = 7.591517e-05, step = 1008 (5.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69912\n",
      "2022-12-23 03:19:59,892 [INFO] tensorflow: global_step/sec: 1.69912\n",
      "2022-12-23 03:20:04,024 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 7/10: loss: 0.00011 learning rate: 0.00050 Time taken: 0:01:26.382765 ETA: 0:04:19.148294\n",
      "INFO:tensorflow:epoch = 7.020689655172414, learning_rate = 0.00048436935, loss = 0.0001416056, step = 1018 (5.915 sec)\n",
      "2022-12-23 03:20:05,806 [INFO] tensorflow: epoch = 7.020689655172414, learning_rate = 0.00048436935, loss = 0.0001416056, step = 1018 (5.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69089\n",
      "2022-12-23 03:20:08,172 [INFO] tensorflow: global_step/sec: 1.69089\n",
      "2022-12-23 03:20:09,363 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.077\n",
      "INFO:tensorflow:epoch = 7.089655172413793, learning_rate = 0.00043571217, loss = 0.00010016604, step = 1028 (5.943 sec)\n",
      "2022-12-23 03:20:11,749 [INFO] tensorflow: epoch = 7.089655172413793, learning_rate = 0.00043571217, loss = 0.00010016604, step = 1028 (5.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.69323\n",
      "2022-12-23 03:20:16,440 [INFO] tensorflow: global_step/sec: 1.69323\n",
      "INFO:tensorflow:epoch = 7.158620689655172, learning_rate = 0.00039194277, loss = 9.221755e-05, step = 1038 (5.887 sec)\n",
      "2022-12-23 03:20:17,636 [INFO] tensorflow: epoch = 7.158620689655172, learning_rate = 0.00039194277, loss = 9.221755e-05, step = 1038 (5.887 sec)\n",
      "INFO:tensorflow:epoch = 7.227586206896552, learning_rate = 0.00035257026, loss = 0.00013883754, step = 1048 (5.991 sec)\n",
      "2022-12-23 03:20:23,627 [INFO] tensorflow: epoch = 7.227586206896552, learning_rate = 0.00035257026, loss = 0.00013883754, step = 1048 (5.991 sec)\n",
      "2022-12-23 03:20:24,208 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.945\n",
      "INFO:tensorflow:global_step/sec: 1.67355\n",
      "2022-12-23 03:20:24,805 [INFO] tensorflow: global_step/sec: 1.67355\n",
      "INFO:tensorflow:epoch = 7.296551724137931, learning_rate = 0.0003171526, loss = 0.00014019107, step = 1058 (6.186 sec)\n",
      "2022-12-23 03:20:29,813 [INFO] tensorflow: epoch = 7.296551724137931, learning_rate = 0.0003171526, loss = 0.00014019107, step = 1058 (6.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.52652\n",
      "2022-12-23 03:20:33,976 [INFO] tensorflow: global_step/sec: 1.52652\n",
      "INFO:tensorflow:epoch = 7.358620689655172, learning_rate = 0.00028832987, loss = 0.00010735938, step = 1067 (5.968 sec)\n",
      "2022-12-23 03:20:35,780 [INFO] tensorflow: epoch = 7.358620689655172, learning_rate = 0.00028832987, loss = 0.00010735938, step = 1067 (5.968 sec)\n",
      "2022-12-23 03:20:39,995 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.339\n",
      "INFO:tensorflow:epoch = 7.427586206896551, learning_rate = 0.00025936565, loss = 0.000117982956, step = 1077 (6.008 sec)\n",
      "2022-12-23 03:20:41,789 [INFO] tensorflow: epoch = 7.427586206896551, learning_rate = 0.00025936565, loss = 0.000117982956, step = 1077 (6.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6662\n",
      "2022-12-23 03:20:42,379 [INFO] tensorflow: global_step/sec: 1.6662\n",
      "INFO:tensorflow:epoch = 7.496551724137931, learning_rate = 0.000233311, loss = 0.00013379785, step = 1087 (5.926 sec)\n",
      "2022-12-23 03:20:47,715 [INFO] tensorflow: epoch = 7.496551724137931, learning_rate = 0.000233311, loss = 0.00013379785, step = 1087 (5.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.68874\n",
      "2022-12-23 03:20:50,669 [INFO] tensorflow: global_step/sec: 1.68874\n",
      "INFO:tensorflow:epoch = 7.56551724137931, learning_rate = 0.0002098739, loss = 0.0001381503, step = 1097 (5.937 sec)\n",
      "2022-12-23 03:20:53,652 [INFO] tensorflow: epoch = 7.56551724137931, learning_rate = 0.0002098739, loss = 0.0001381503, step = 1097 (5.937 sec)\n",
      "2022-12-23 03:20:54,831 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.961\n",
      "INFO:tensorflow:global_step/sec: 1.69623\n",
      "2022-12-23 03:20:58,923 [INFO] tensorflow: global_step/sec: 1.69623\n",
      "INFO:tensorflow:epoch = 7.634482758620689, learning_rate = 0.00018879096, loss = 0.00015817457, step = 1107 (5.873 sec)\n",
      "2022-12-23 03:20:59,525 [INFO] tensorflow: epoch = 7.634482758620689, learning_rate = 0.00018879096, loss = 0.00015817457, step = 1107 (5.873 sec)\n",
      "INFO:tensorflow:epoch = 7.703448275862069, learning_rate = 0.00016982593, loss = 0.00013800518, step = 1117 (6.032 sec)\n",
      "2022-12-23 03:21:05,556 [INFO] tensorflow: epoch = 7.703448275862069, learning_rate = 0.00016982593, loss = 0.00013800518, step = 1117 (6.032 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66624\n",
      "2022-12-23 03:21:07,325 [INFO] tensorflow: global_step/sec: 1.66624\n",
      "2022-12-23 03:21:09,693 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.915\n",
      "INFO:tensorflow:epoch = 7.772413793103448, learning_rate = 0.00015276618, loss = 9.386433e-05, step = 1127 (5.976 sec)\n",
      "2022-12-23 03:21:11,532 [INFO] tensorflow: epoch = 7.772413793103448, learning_rate = 0.00015276618, loss = 9.386433e-05, step = 1127 (5.976 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67145\n",
      "2022-12-23 03:21:15,701 [INFO] tensorflow: global_step/sec: 1.67145\n",
      "INFO:tensorflow:epoch = 7.841379310344827, learning_rate = 0.00013742015, loss = 0.000107272426, step = 1137 (5.988 sec)\n",
      "2022-12-23 03:21:17,520 [INFO] tensorflow: epoch = 7.841379310344827, learning_rate = 0.00013742015, loss = 0.000107272426, step = 1137 (5.988 sec)\n",
      "INFO:tensorflow:epoch = 7.910344827586207, learning_rate = 0.00012361558, loss = 0.00011562236, step = 1147 (6.006 sec)\n",
      "2022-12-23 03:21:23,526 [INFO] tensorflow: epoch = 7.910344827586207, learning_rate = 0.00012361558, loss = 0.00011562236, step = 1147 (6.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66271\n",
      "2022-12-23 03:21:24,121 [INFO] tensorflow: global_step/sec: 1.66271\n",
      "2022-12-23 03:21:24,722 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.616\n",
      "INFO:tensorflow:epoch = 7.979310344827586, learning_rate = 0.000111197754, loss = 9.0672234e-05, step = 1157 (5.977 sec)\n",
      "2022-12-23 03:21:29,503 [INFO] tensorflow: epoch = 7.979310344827586, learning_rate = 0.000111197754, loss = 9.0672234e-05, step = 1157 (5.977 sec)\n",
      "2022-12-23 03:21:31,325 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 8/10: loss: 0.00011 learning rate: 0.00011 Time taken: 0:01:27.309027 ETA: 0:02:54.618054\n",
      "INFO:tensorflow:global_step/sec: 1.67215\n",
      "2022-12-23 03:21:32,493 [INFO] tensorflow: global_step/sec: 1.67215\n",
      "INFO:tensorflow:epoch = 8.048275862068966, learning_rate = 0.00010002746, loss = 6.529055e-05, step = 1167 (6.117 sec)\n",
      "2022-12-23 03:21:35,620 [INFO] tensorflow: epoch = 8.048275862068966, learning_rate = 0.00010002746, loss = 6.529055e-05, step = 1167 (6.117 sec)\n",
      "2022-12-23 03:21:40,418 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.484\n",
      "INFO:tensorflow:epoch = 8.110344827586207, learning_rate = 9.0936825e-05, loss = 9.851738e-05, step = 1176 (5.991 sec)\n",
      "2022-12-23 03:21:41,611 [INFO] tensorflow: epoch = 8.110344827586207, learning_rate = 9.0936825e-05, loss = 9.851738e-05, step = 1176 (5.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.53524\n",
      "2022-12-23 03:21:41,612 [INFO] tensorflow: global_step/sec: 1.53524\n",
      "INFO:tensorflow:epoch = 8.179310344827586, learning_rate = 8.180182e-05, loss = 8.0507336e-05, step = 1186 (6.019 sec)\n",
      "2022-12-23 03:21:47,630 [INFO] tensorflow: epoch = 8.179310344827586, learning_rate = 8.180182e-05, loss = 8.0507336e-05, step = 1186 (6.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.6765\n",
      "2022-12-23 03:21:49,963 [INFO] tensorflow: global_step/sec: 1.6765\n",
      "INFO:tensorflow:epoch = 8.248275862068965, learning_rate = 7.358441e-05, loss = 0.00010833655, step = 1196 (5.935 sec)\n",
      "2022-12-23 03:21:53,566 [INFO] tensorflow: epoch = 8.248275862068965, learning_rate = 7.358441e-05, loss = 0.00010833655, step = 1196 (5.935 sec)\n",
      "2022-12-23 03:21:55,329 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.827\n",
      "INFO:tensorflow:global_step/sec: 1.67723\n",
      "2022-12-23 03:21:58,310 [INFO] tensorflow: global_step/sec: 1.67723\n",
      "INFO:tensorflow:epoch = 8.317241379310344, learning_rate = 6.619247e-05, loss = 7.568675e-05, step = 1206 (5.965 sec)\n",
      "2022-12-23 03:21:59,531 [INFO] tensorflow: epoch = 8.317241379310344, learning_rate = 6.619247e-05, loss = 7.568675e-05, step = 1206 (5.965 sec)\n",
      "INFO:tensorflow:epoch = 8.386206896551723, learning_rate = 5.9543145e-05, loss = 9.2300776e-05, step = 1216 (6.037 sec)\n",
      "2022-12-23 03:22:05,568 [INFO] tensorflow: epoch = 8.386206896551723, learning_rate = 5.9543145e-05, loss = 9.2300776e-05, step = 1216 (6.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.65595\n",
      "2022-12-23 03:22:06,764 [INFO] tensorflow: global_step/sec: 1.65595\n",
      "2022-12-23 03:22:10,413 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.519\n",
      "INFO:tensorflow:epoch = 8.455172413793104, learning_rate = 5.356178e-05, loss = 0.00013005498, step = 1226 (6.056 sec)\n",
      "2022-12-23 03:22:11,624 [INFO] tensorflow: epoch = 8.455172413793104, learning_rate = 5.356178e-05, loss = 0.00013005498, step = 1226 (6.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.64962\n",
      "2022-12-23 03:22:15,251 [INFO] tensorflow: global_step/sec: 1.64962\n",
      "INFO:tensorflow:epoch = 8.524137931034483, learning_rate = 4.818122e-05, loss = 0.00011606662, step = 1236 (6.009 sec)\n",
      "2022-12-23 03:22:17,633 [INFO] tensorflow: epoch = 8.524137931034483, learning_rate = 4.818122e-05, loss = 0.00011606662, step = 1236 (6.009 sec)\n",
      "INFO:tensorflow:epoch = 8.593103448275862, learning_rate = 4.3341166e-05, loss = 8.055847e-05, step = 1246 (6.058 sec)\n",
      "2022-12-23 03:22:23,692 [INFO] tensorflow: epoch = 8.593103448275862, learning_rate = 4.3341166e-05, loss = 8.055847e-05, step = 1246 (6.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.65852\n",
      "2022-12-23 03:22:23,692 [INFO] tensorflow: global_step/sec: 1.65852\n",
      "2022-12-23 03:22:25,502 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.510\n",
      "INFO:tensorflow:epoch = 8.662068965517241, learning_rate = 3.8987357e-05, loss = 0.00012300399, step = 1256 (5.971 sec)\n",
      "2022-12-23 03:22:29,662 [INFO] tensorflow: epoch = 8.662068965517241, learning_rate = 3.8987357e-05, loss = 0.00012300399, step = 1256 (5.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.66322\n",
      "2022-12-23 03:22:32,110 [INFO] tensorflow: global_step/sec: 1.66322\n",
      "INFO:tensorflow:epoch = 8.73103448275862, learning_rate = 3.5070872e-05, loss = 8.333892e-05, step = 1266 (6.091 sec)\n",
      "2022-12-23 03:22:35,753 [INFO] tensorflow: epoch = 8.73103448275862, learning_rate = 3.5070872e-05, loss = 8.333892e-05, step = 1266 (6.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.67494\n",
      "2022-12-23 03:22:40,468 [INFO] tensorflow: global_step/sec: 1.67494\n",
      "2022-12-23 03:22:40,469 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.726\n",
      "INFO:tensorflow:epoch = 8.8, learning_rate = 3.154782e-05, loss = 8.8669505e-05, step = 1276 (6.046 sec)\n",
      "2022-12-23 03:22:41,800 [INFO] tensorflow: epoch = 8.8, learning_rate = 3.154782e-05, loss = 8.8669505e-05, step = 1276 (6.046 sec)\n",
      "INFO:tensorflow:epoch = 8.862068965517242, learning_rate = 2.868074e-05, loss = 0.000113722184, step = 1285 (6.093 sec)\n",
      "2022-12-23 03:22:47,892 [INFO] tensorflow: epoch = 8.862068965517242, learning_rate = 2.868074e-05, loss = 0.000113722184, step = 1285 (6.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.50649\n",
      "2022-12-23 03:22:49,761 [INFO] tensorflow: global_step/sec: 1.50649\n",
      "INFO:tensorflow:epoch = 8.931034482758621, learning_rate = 2.5799634e-05, loss = 9.4825074e-05, step = 1295 (5.972 sec)\n",
      "2022-12-23 03:22:53,864 [INFO] tensorflow: epoch = 8.931034482758621, learning_rate = 2.5799634e-05, loss = 9.4825074e-05, step = 1295 (5.972 sec)\n",
      "2022-12-23 03:22:56,234 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 25.373\n",
      "INFO:tensorflow:global_step/sec: 1.69786\n",
      "2022-12-23 03:22:58,007 [INFO] tensorflow: global_step/sec: 1.69786\n",
      "INFO:tensorflow:epoch = 9.0, learning_rate = 2.3207927e-05, loss = 0.00014283316, step = 1305 (5.899 sec)\n",
      "2022-12-23 03:22:59,763 [INFO] tensorflow: epoch = 9.0, learning_rate = 2.3207927e-05, loss = 0.00014283316, step = 1305 (5.899 sec)\n",
      "2022-12-23 03:22:59,764 [INFO] iva.detectnet_v2.tfhooks.task_progress_monitor_hook: Epoch 9/10: loss: 0.00014 learning rate: 0.00002 Time taken: 0:01:28.487601 ETA: 0:01:28.487601\n",
      "INFO:tensorflow:epoch = 9.068965517241379, learning_rate = 2.0876589e-05, loss = 8.585154e-05, step = 1315 (5.803 sec)\n",
      "2022-12-23 03:23:05,567 [INFO] tensorflow: epoch = 9.068965517241379, learning_rate = 2.0876589e-05, loss = 8.585154e-05, step = 1315 (5.803 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71933\n",
      "2022-12-23 03:23:06,150 [INFO] tensorflow: global_step/sec: 1.71933\n",
      "2022-12-23 03:23:10,840 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.386\n",
      "INFO:tensorflow:epoch = 9.137931034482758, learning_rate = 1.8779427e-05, loss = 8.653608e-05, step = 1325 (5.863 sec)\n",
      "2022-12-23 03:23:11,430 [INFO] tensorflow: epoch = 9.137931034482758, learning_rate = 1.8779427e-05, loss = 8.653608e-05, step = 1325 (5.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70351\n",
      "2022-12-23 03:23:14,368 [INFO] tensorflow: global_step/sec: 1.70351\n",
      "INFO:tensorflow:epoch = 9.206896551724137, learning_rate = 1.6892936e-05, loss = 9.667259e-05, step = 1335 (5.854 sec)\n",
      "2022-12-23 03:23:17,284 [INFO] tensorflow: epoch = 9.206896551724137, learning_rate = 1.6892936e-05, loss = 9.667259e-05, step = 1335 (5.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70301\n",
      "2022-12-23 03:23:22,589 [INFO] tensorflow: global_step/sec: 1.70301\n",
      "INFO:tensorflow:epoch = 9.275862068965518, learning_rate = 1.5195966e-05, loss = 8.166046e-05, step = 1345 (5.892 sec)\n",
      "2022-12-23 03:23:23,176 [INFO] tensorflow: epoch = 9.275862068965518, learning_rate = 1.5195966e-05, loss = 8.166046e-05, step = 1345 (5.892 sec)\n",
      "2022-12-23 03:23:25,525 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.240\n",
      "INFO:tensorflow:epoch = 9.344827586206897, learning_rate = 1.36694525e-05, loss = 0.00012540698, step = 1355 (5.869 sec)\n",
      "2022-12-23 03:23:29,045 [INFO] tensorflow: epoch = 9.344827586206897, learning_rate = 1.36694525e-05, loss = 0.00012540698, step = 1355 (5.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70278\n",
      "2022-12-23 03:23:30,811 [INFO] tensorflow: global_step/sec: 1.70278\n",
      "INFO:tensorflow:epoch = 9.413793103448276, learning_rate = 1.22962965e-05, loss = 8.254091e-05, step = 1365 (5.862 sec)\n",
      "2022-12-23 03:23:34,907 [INFO] tensorflow: epoch = 9.413793103448276, learning_rate = 1.22962965e-05, loss = 8.254091e-05, step = 1365 (5.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.70415\n",
      "2022-12-23 03:23:39,026 [INFO] tensorflow: global_step/sec: 1.70415\n",
      "2022-12-23 03:23:40,176 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.301\n",
      "INFO:tensorflow:epoch = 9.482758620689655, learning_rate = 1.106108e-05, loss = 0.0001240089, step = 1375 (5.852 sec)\n",
      "2022-12-23 03:23:40,759 [INFO] tensorflow: epoch = 9.482758620689655, learning_rate = 1.106108e-05, loss = 0.0001240089, step = 1375 (5.852 sec)\n",
      "INFO:tensorflow:epoch = 9.551724137931034, learning_rate = 9.949937e-06, loss = 0.00010217739, step = 1385 (5.797 sec)\n",
      "2022-12-23 03:23:46,556 [INFO] tensorflow: epoch = 9.551724137931034, learning_rate = 9.949937e-06, loss = 0.00010217739, step = 1385 (5.797 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72847\n",
      "2022-12-23 03:23:47,126 [INFO] tensorflow: global_step/sec: 1.72847\n",
      "INFO:tensorflow:epoch = 9.620689655172413, learning_rate = 8.950423e-06, loss = 8.147793e-05, step = 1395 (6.274 sec)\n",
      "2022-12-23 03:23:52,830 [INFO] tensorflow: epoch = 9.620689655172413, learning_rate = 8.950423e-06, loss = 8.147793e-05, step = 1395 (6.274 sec)\n",
      "2022-12-23 03:23:55,464 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 26.166\n",
      "INFO:tensorflow:global_step/sec: 1.56801\n",
      "2022-12-23 03:23:56,054 [INFO] tensorflow: global_step/sec: 1.56801\n",
      "INFO:tensorflow:epoch = 9.689655172413794, learning_rate = 8.051306e-06, loss = 0.0001499085, step = 1405 (6.152 sec)\n",
      "2022-12-23 03:23:58,982 [INFO] tensorflow: epoch = 9.689655172413794, learning_rate = 8.051306e-06, loss = 0.0001499085, step = 1405 (6.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.71536\n",
      "2022-12-23 03:24:04,216 [INFO] tensorflow: global_step/sec: 1.71536\n",
      "INFO:tensorflow:epoch = 9.758620689655173, learning_rate = 7.242517e-06, loss = 8.417512e-05, step = 1415 (5.819 sec)\n",
      "2022-12-23 03:24:04,801 [INFO] tensorflow: epoch = 9.758620689655173, learning_rate = 7.242517e-06, loss = 8.417512e-05, step = 1415 (5.819 sec)\n",
      "2022-12-23 03:24:09,973 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 27.569\n",
      "INFO:tensorflow:epoch = 9.827586206896552, learning_rate = 6.5149684e-06, loss = 8.927038e-05, step = 1425 (5.735 sec)\n",
      "2022-12-23 03:24:10,537 [INFO] tensorflow: epoch = 9.827586206896552, learning_rate = 6.5149684e-06, loss = 8.927038e-05, step = 1425 (5.735 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72233\n",
      "2022-12-23 03:24:12,344 [INFO] tensorflow: global_step/sec: 1.72233\n",
      "INFO:tensorflow:epoch = 9.89655172413793, learning_rate = 5.860506e-06, loss = 8.778143e-05, step = 1435 (5.908 sec)\n",
      "2022-12-23 03:24:16,445 [INFO] tensorflow: epoch = 9.89655172413793, learning_rate = 5.860506e-06, loss = 8.778143e-05, step = 1435 (5.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.72226\n",
      "2022-12-23 03:24:20,473 [INFO] tensorflow: global_step/sec: 1.72226\n",
      "INFO:tensorflow:epoch = 9.96551724137931, learning_rate = 5.271793e-06, loss = 0.0001141757, step = 1445 (5.761 sec)\n",
      "2022-12-23 03:24:22,206 [INFO] tensorflow: epoch = 9.96551724137931, learning_rate = 5.271793e-06, loss = 0.0001141757, step = 1445 (5.761 sec)\n",
      "INFO:tensorflow:Saving checkpoints for step-1450.\n",
      "2022-12-23 03:24:24,537 [INFO] tensorflow: Saving checkpoints for step-1450.\n",
      "2022-12-23 03:24:26,212 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 36, 0.00s/step\n",
      "2022-12-23 03:24:57,638 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 36, 3.14s/step\n",
      "2022-12-23 03:25:29,672 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 36, 3.20s/step\n",
      "2022-12-23 03:26:01,224 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 36, 3.16s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 9773/9773 [00:00<00:00, 18757.45it/s]\n",
      "Epoch 10/10\n",
      "=========================\n",
      "\n",
      "Validation cost: 0.000069\n",
      "Mean average_precision (in %): 46.7932\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                              46.7932\n",
      "\n",
      "Median Inference Time: 0.019179\n",
      "2022-12-23 03:26:21,516 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 3.041\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-12-23 03:26:21,757 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-12-23 03:26:21,757 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-12-23 03:26:21,759 [INFO] modulus.hooks.sample_counter_hook: Train Samples / sec: 3.041\n",
      "Time taken to run __main__:main: 0:20:23.821091.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Initiate the training process\n",
    "!detectnet_v2 train -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                    -r $MODELS_DIR/resnet18_detector_pruned_retrained_qat \\\n",
    "                    -k tlt_encode \\\n",
    "                    -n resnet18_detector_pruned_retrained_qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b81a8b55-a11a-4b03-b723-1c97dbc42a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 7112\n",
      "-rw-r--r-- 1 root root 7280552 Dec 23 03:26 resnet18_detector_pruned_retrained_qat.tlt\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# List the newly retrained model\n",
    "!ls -rlt $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765e20d-9fb7-4a82-ae13-66a40feac5c2",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "### Evaluate Retrained Model ###\n",
    "Once the retraining is complete, we can evaluate the QAT enabled pruned retrained model. The mAP (mean average precision) of this model should be comparable to that of the unpruned model (without QAT). However, due to quantization, it is possible sometimes to see a drop in the mAP value. Pruning and retraining can be an iterative process, but the TAO Toolkit makes it easy to rapidly prototype different versions of the video AI model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643daab1-33c3-4be9-a1af-9dd284c3938a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:43: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "2022-12-23 03:31:16,093 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /dli/task/spec_files/combined_training_config_qat.txt\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "2022-12-23 03:31:16,097 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:153: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "2022-12-23 03:31:16,199 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "2022-12-23 03:31:16,214 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "2022-12-23 03:31:16,238 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "2022-12-23 03:31:17,284 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "2022-12-23 03:31:17,284 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "2022-12-23 03:31:17,284 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "2022-12-23 03:31:17,761 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "2022-12-23 03:31:17,761 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2022-12-23 03:31:18,219 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "2022-12-23 03:31:18,544 [INFO] iva.detectnet_v2.objectives.bbox_objective: Default L1 loss function will be used.\n",
      "2022-12-23 03:31:18,846 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Serial augmentation enabled = False\n",
      "2022-12-23 03:31:18,847 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Pseudo sharding enabled = False\n",
      "2022-12-23 03:31:18,847 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: Max Image Dimensions (all sources): (0, 0)\n",
      "2022-12-23 03:31:18,847 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: number of cpus: 4, io threads: 8, compute threads: 4, buffered batches: 4\n",
      "2022-12-23 03:31:18,847 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: total dataset size 578, number of sources: 1, batch size per gpu: 16, steps: 37\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "2022-12-23 03:31:18,881 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f5e7522e6d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f5e7522e6d8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:31:18,919 [WARNING] tensorflow: Entity <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f5e7522e6d8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method DriveNetTFRecordsParser.__call__ of <iva.detectnet_v2.dataloader.drivenet_dataloader.DriveNetTFRecordsParser object at 0x7f5e7522e6d8>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:31:18,939 [INFO] iva.detectnet_v2.dataloader.default_dataloader: Bounding box coordinates were detected in the input specification! Bboxes will be automatically converted to polygon coordinates.\n",
      "2022-12-23 03:31:19,191 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: shuffle: False - shard 0 of 1\n",
      "2022-12-23 03:31:19,197 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: sampling 1 datasets with weights:\n",
      "2022-12-23 03:31:19,197 [INFO] modulus.blocks.data_loaders.multi_source_loader.data_loader: source: 0 weight: 1.000000\n",
      "WARNING:tensorflow:Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5e58326550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5e58326550>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:31:19,212 [WARNING] tensorflow: Entity <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5e58326550>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Unable to locate the source code of <bound method Processor.__call__ of <modulus.blocks.data_loaders.multi_source_loader.processors.asset_loader.AssetLoader object at 0x7f5e58326550>>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\n",
      "2022-12-23 03:31:19,466 [INFO] iva.detectnet_v2.evaluation.build_evaluator: Found 578 samples in validation set\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "2022-12-23 03:31:19,467 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:107: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "2022-12-23 03:31:19,468 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:110: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "2022-12-23 03:31:19,470 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:113: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "2022-12-23 03:31:19,598 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/rasterizers/bbox_rasterizer.py:347: The name tf.bincount is deprecated. Please use tf.math.bincount instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "2022-12-23 03:31:20,045 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_functions.py:17: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "2022-12-23 03:31:20,055 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/cost_function/cost_auto_weight_hook.py:235: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 544, 960)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1_qdq (QDQ)               (None, 3, 544, 960)  1           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (QuantizedConv2D)         (None, 24, 272, 480) 3552        input_1_qdq[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 24, 272, 480) 96          conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (ReLU)             (None, 24, 272, 480) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1_qdq (QDQ)          (None, 24, 272, 480) 1           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_1 (QuantizedConv2 (None, 48, 136, 240) 10416       activation_1_qdq[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_1 (BatchNormalizati (None, 48, 136, 240) 192         block_1a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1 (ReLU)          (None, 48, 136, 240) 0           block_1a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_1_qdq (QDQ)       (None, 48, 136, 240) 1           block_1a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_2 (QuantizedConv2 (None, 64, 136, 240) 27712       block_1a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_conv_shortcut (Quantiz (None, 64, 136, 240) 1600        activation_1_qdq[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut (BatchNorm (None, 64, 136, 240) 256         block_1a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_2_qdq (QDQ)         (None, 64, 136, 240) 1           block_1a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_bn_shortcut_qdq (QDQ)  (None, 64, 136, 240) 1           block_1a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 136, 240) 0           block_1a_bn_2_qdq[0][0]          \n",
      "                                                                 block_1a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_1_qdq (QDQ)                 (None, 64, 136, 240) 1           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu (ReLU)            (None, 64, 136, 240) 0           add_1_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_1a_relu_qdq (QDQ)         (None, 64, 136, 240) 1           block_1a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_1 (QuantizedConv2 (None, 64, 136, 240) 36928       block_1a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_1 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1 (ReLU)          (None, 64, 136, 240) 0           block_1b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_1_qdq (QDQ)       (None, 64, 136, 240) 1           block_1b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_conv_2 (QuantizedConv2 (None, 64, 136, 240) 36928       block_1b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2 (BatchNormalizati (None, 64, 136, 240) 256         block_1b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_bn_2_qdq (QDQ)         (None, 64, 136, 240) 1           block_1b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 136, 240) 0           block_1b_bn_2_qdq[0][0]          \n",
      "                                                                 block_1a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_2_qdq (QDQ)                 (None, 64, 136, 240) 1           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu (ReLU)            (None, 64, 136, 240) 0           add_2_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_1b_relu_qdq (QDQ)         (None, 64, 136, 240) 1           block_1b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_1 (QuantizedConv2 (None, 80, 68, 120)  46160       block_1b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_1 (BatchNormalizati (None, 80, 68, 120)  320         block_2a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1 (ReLU)          (None, 80, 68, 120)  0           block_2a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_1_qdq (QDQ)       (None, 80, 68, 120)  1           block_2a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_2 (QuantizedConv2 (None, 112, 68, 120) 80752       block_2a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_conv_shortcut (Quantiz (None, 112, 68, 120) 7280        block_1b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut (BatchNorm (None, 112, 68, 120) 448         block_2a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_2_qdq (QDQ)         (None, 112, 68, 120) 1           block_2a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_bn_shortcut_qdq (QDQ)  (None, 112, 68, 120) 1           block_2a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 112, 68, 120) 0           block_2a_bn_2_qdq[0][0]          \n",
      "                                                                 block_2a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_3_qdq (QDQ)                 (None, 112, 68, 120) 1           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu (ReLU)            (None, 112, 68, 120) 0           add_3_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_2a_relu_qdq (QDQ)         (None, 112, 68, 120) 1           block_2a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_1 (QuantizedConv2 (None, 96, 68, 120)  96864       block_2a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_1 (BatchNormalizati (None, 96, 68, 120)  384         block_2b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1 (ReLU)          (None, 96, 68, 120)  0           block_2b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_1_qdq (QDQ)       (None, 96, 68, 120)  1           block_2b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_conv_2 (QuantizedConv2 (None, 112, 68, 120) 96880       block_2b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2 (BatchNormalizati (None, 112, 68, 120) 448         block_2b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_bn_2_qdq (QDQ)         (None, 112, 68, 120) 1           block_2b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 112, 68, 120) 0           block_2b_bn_2_qdq[0][0]          \n",
      "                                                                 block_2a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_4_qdq (QDQ)                 (None, 112, 68, 120) 1           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu (ReLU)            (None, 112, 68, 120) 0           add_4_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_2b_relu_qdq (QDQ)         (None, 112, 68, 120) 1           block_2b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_1 (QuantizedConv2 (None, 104, 34, 60)  104936      block_2b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_1 (BatchNormalizati (None, 104, 34, 60)  416         block_3a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1 (ReLU)          (None, 104, 34, 60)  0           block_3a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_1_qdq (QDQ)       (None, 104, 34, 60)  1           block_3a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_2 (QuantizedConv2 (None, 192, 34, 60)  179904      block_3a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_conv_shortcut (Quantiz (None, 192, 34, 60)  21696       block_2b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_3a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_3a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_bn_shortcut_qdq (QDQ)  (None, 192, 34, 60)  1           block_3a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 192, 34, 60)  0           block_3a_bn_2_qdq[0][0]          \n",
      "                                                                 block_3a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_5_qdq (QDQ)                 (None, 192, 34, 60)  1           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu (ReLU)            (None, 192, 34, 60)  0           add_5_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_3a_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_3a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_1 (QuantizedConv2 (None, 96, 34, 60)   165984      block_3a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_1 (BatchNormalizati (None, 96, 34, 60)   384         block_3b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1 (ReLU)          (None, 96, 34, 60)   0           block_3b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_1_qdq (QDQ)       (None, 96, 34, 60)   1           block_3b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_conv_2 (QuantizedConv2 (None, 192, 34, 60)  166080      block_3b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_3b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_3b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 192, 34, 60)  0           block_3b_bn_2_qdq[0][0]          \n",
      "                                                                 block_3a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_6_qdq (QDQ)                 (None, 192, 34, 60)  1           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu (ReLU)            (None, 192, 34, 60)  0           add_6_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_3b_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_3b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_1 (QuantizedConv2 (None, 120, 34, 60)  207480      block_3b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_1 (BatchNormalizati (None, 120, 34, 60)  480         block_4a_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1 (ReLU)          (None, 120, 34, 60)  0           block_4a_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_1_qdq (QDQ)       (None, 120, 34, 60)  1           block_4a_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_2 (QuantizedConv2 (None, 192, 34, 60)  207552      block_4a_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_conv_shortcut (Quantiz (None, 192, 34, 60)  37056       block_3b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4a_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut (BatchNorm (None, 192, 34, 60)  768         block_4a_conv_shortcut[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_4a_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_bn_shortcut_qdq (QDQ)  (None, 192, 34, 60)  1           block_4a_bn_shortcut[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 192, 34, 60)  0           block_4a_bn_2_qdq[0][0]          \n",
      "                                                                 block_4a_bn_shortcut_qdq[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_7_qdq (QDQ)                 (None, 192, 34, 60)  1           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu (ReLU)            (None, 192, 34, 60)  0           add_7_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_4a_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_4a_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_1 (QuantizedConv2 (None, 56, 34, 60)   96824       block_4a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_1 (BatchNormalizati (None, 56, 34, 60)   224         block_4b_conv_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1 (ReLU)          (None, 56, 34, 60)   0           block_4b_bn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_1_qdq (QDQ)       (None, 56, 34, 60)   1           block_4b_relu_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_conv_2 (QuantizedConv2 (None, 192, 34, 60)  96960       block_4b_relu_1_qdq[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2 (BatchNormalizati (None, 192, 34, 60)  768         block_4b_conv_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_bn_2_qdq (QDQ)         (None, 192, 34, 60)  1           block_4b_bn_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 192, 34, 60)  0           block_4b_bn_2_qdq[0][0]          \n",
      "                                                                 block_4a_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_8_qdq (QDQ)                 (None, 192, 34, 60)  1           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu (ReLU)            (None, 192, 34, 60)  0           add_8_qdq[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "block_4b_relu_qdq (QDQ)         (None, 192, 34, 60)  1           block_4b_relu[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "output_bbox (Conv2D)            (None, 4, 34, 60)    772         block_4b_relu_qdq[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "output_cov (Conv2D)             (None, 1, 34, 60)    193         block_4b_relu_qdq[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 1,740,019\n",
      "Trainable params: 1,618,109\n",
      "Non-trainable params: 121,910\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "2022-12-23 03:31:20,066 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:139: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "2022-12-23 03:31:20,066 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:14: The name tf.local_variables_initializer is deprecated. Please use tf.compat.v1.local_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "2022-12-23 03:31:20,066 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:15: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "2022-12-23 03:31:20,067 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/common/graph/initializers.py:16: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "2022-12-23 03:31:20,067 [WARNING] tensorflow: From /root/.cache/bazel/_bazel_root/ed34e6d125608f91724fda23656f1726/execroot/ai_infra/bazel-out/k8-fastbuild/bin/magnet/packages/iva/build_wheel.runfiles/ai_infra/iva/detectnet_v2/training/utilities.py:140: The name tf.train.SingularMonitoredSession is deprecated. Please use tf.compat.v1.train.SingularMonitoredSession instead.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2022-12-23 03:31:20,533 [INFO] tensorflow: Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "2022-12-23 03:31:21,006 [INFO] tensorflow: Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2022-12-23 03:31:21,362 [INFO] tensorflow: Done running local_init_op.\n",
      "2022-12-23 03:31:22,179 [INFO] iva.detectnet_v2.evaluation.evaluation: step 0 / 37, 0.00s/step\n",
      "2022-12-23 03:32:00,017 [INFO] iva.detectnet_v2.evaluation.evaluation: step 10 / 37, 3.78s/step\n",
      "2022-12-23 03:32:31,305 [INFO] iva.detectnet_v2.evaluation.evaluation: step 20 / 37, 3.13s/step\n",
      "2022-12-23 03:33:03,308 [INFO] iva.detectnet_v2.evaluation.evaluation: step 30 / 37, 3.20s/step\n",
      "Matching predictions to ground truth, class 1/1.: 100%|█| 10039/10039 [00:00<00:00, 17728.96it/s]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "2022-12-23 03:33:25,978 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "2022-12-23 03:33:25,978 [WARNING] tensorflow: From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "\n",
      "Validation cost: 0.001399\n",
      "Mean average_precision (in %): 46.7596\n",
      "\n",
      "class name      average precision (in %)\n",
      "------------  --------------------------\n",
      "car                              46.7596\n",
      "\n",
      "Median Inference Time: 0.018170\n",
      "2022-12-23 03:33:26,027 [INFO] __main__: Evaluation complete.\n",
      "Time taken to run __main__:main: 0:02:09.934833.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Evaluate the model using the same validation set as training\n",
    "!detectnet_v2 evaluate -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                       -m $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights/resnet18_detector_pruned_retrained_qat.tlt \\\n",
    "                       -k tlt_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839418c3-5129-439b-85c9-59963eab72f2",
   "metadata": {},
   "source": [
    "<a name='s4'></a>\n",
    "## Export Model with Calibration Cache ##\n",
    "When we feel confident in the model's accuracy as well as inference performance, it can be exported to integrate into DeepStream. To enable inference at lower precision for better performance, the **TensorRT engine** needs to be generated in INT8 mode. This process requires an additional **cache file** that contains scale factors to help combat quantization errors, which may arise due to low-precision arithmetic. The calibration cache can optionally be created with the `export` subtask. This is referred to as exporting in **INT8 mode**. When using the `export` subtask, we can include the `--cal_cache_file` argument to indicate the path to save the calibration cache file to and the `--data_type int8` argument to indicate the desired data type. The options for the `--data_type` argument are `fp32`, `fp16`, and `int8`. The default value is `fp32` if inference in INT8 mode is not required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4536043-cba1-4bfa-b254-34da1e3dd82c",
   "metadata": {},
   "source": [
    "Execute the below cell to export the QAT trained model. This command generates an `.etlt` file from the trained model and serializes the corresponding INT8 scales as a TensorRT readable calibration cache file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25067f37-32e7-41be-baaa-5eb5d224d69f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n",
      "2022-12-23 03:35:07,683 [INFO] iva.detectnet_v2.spec_handler.spec_loader: Merging specification from /dli/task/spec_files/combined_training_config_qat.txt\n",
      "2022-12-23 03:35:09,713 [INFO] iva.common.export.keras_exporter: Using input nodes: ['input_1']\n",
      "2022-12-23 03:35:09,713 [INFO] iva.common.export.keras_exporter: Using output nodes: ['output_cov/Sigmoid', 'output_bbox/BiasAdd']\n",
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "NOTE: UFF has been tested with TensorFlow 1.14.0.\n",
      "WARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\n",
      "DEBUG [/usr/local/lib/python3.6/dist-packages/uff/converters/tensorflow/converter.py:96] Marking ['output_cov/Sigmoid', 'output_bbox/BiasAdd'] as outputs\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Delete duplicate copies\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_final/resnet18_detector_pruned_retrained_qat.etlt\n",
    "!rm -rf $MODELS_DIR/resnet18_detector_final/cal.bin\n",
    "\n",
    "# Export the QAT trained model\n",
    "!detectnet_v2 export -m $MODELS_DIR/resnet18_detector_pruned_retrained_qat/weights/resnet18_detector_pruned_retrained_qat.tlt \\\n",
    "                     -e $SPEC_FILES_DIR/combined_training_config_qat.txt \\\n",
    "                     -o $MODELS_DIR/resnet18_detector_final/resnet18_detector_pruned_retrained_qat.etlt \\\n",
    "                     -k tlt_encode \\\n",
    "                     --cal_cache_file $MODELS_DIR/resnet18_detector_final/cal.bin \\\n",
    "                     --data_type int8 \\\n",
    "                     --gen_ds_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9a72d-ad8f-411f-a118-99b90505e678",
   "metadata": {},
   "source": [
    "<a name='s5'></a>\n",
    "### Deployment to DeepStream ###\n",
    "The pruned, QAT retrained model is ready to be deployed to DeepStream. We are now able to use `network-mode=1` for INT8 mode in the configuration file for `Gst-nvinfer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268a8da-025f-4db9-9f19-502d4ce1428e",
   "metadata": {},
   "source": [
    "Execute the below cells to read the modified `Gst-nvinfer config file` and pass it to `app_04.py` to run the DeepStream pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8e5893-326b-42ee-9add-b851b1a839b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Pipeline\n",
      "Warning: 'input-dims' parameter has been deprecated. Use 'infer-dims' instead.\n",
      "WARNING: Overriding infer-config batch-size 1  with number of sources  16\n",
      "Adding elements to Pipeline\n",
      "Linking elements in the Pipeline\n",
      "Now playing...\n",
      "1 :  /dli/task/data/sample_30.h264\n",
      "2 :  /dli/task/data/sample_30.h264\n",
      "3 :  /dli/task/data/sample_30.h264\n",
      "4 :  /dli/task/data/sample_30.h264\n",
      "5 :  /dli/task/data/sample_30.h264\n",
      "6 :  /dli/task/data/sample_30.h264\n",
      "7 :  /dli/task/data/sample_30.h264\n",
      "8 :  /dli/task/data/sample_30.h264\n",
      "9 :  /dli/task/data/sample_30.h264\n",
      "10 :  /dli/task/data/sample_30.h264\n",
      "11 :  /dli/task/data/sample_30.h264\n",
      "12 :  /dli/task/data/sample_30.h264\n",
      "13 :  /dli/task/data/sample_30.h264\n",
      "14 :  /dli/task/data/sample_30.h264\n",
      "15 :  /dli/task/data/sample_30.h264\n",
      "16 :  /dli/task/data/sample_30.h264\n",
      "Starting pipeline\n",
      "0:00:00.274664384 \u001b[334m 2349\u001b[00m      0x34458d0 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:635:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::initialize() <nvdsinfer_context_impl.cpp:1161> [UID = 1]: Warning, OpenCV has been deprecated. Using NMS for clustering instead of cv::groupRectangles with topK = 20 and NMS Threshold = 0.5\n",
      "0:00:00.274806158 \u001b[334m 2349\u001b[00m      0x34458d0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() <nvdsinfer_context_impl.cpp:1914> [UID = 1]: Trying to create engine from model files\n",
      "WARNING: ../nvdsinfer/nvdsinfer_model_builder.cpp:661 INT8 calibration file not specified/accessible. INT8 calibration can be done through setDynamicRange API in 'NvDsInferCreateNetwork' implementation\n",
      "WARNING: ../nvdsinfer/nvdsinfer_model_builder.cpp:1204 INT8 calibration file not specified. Trying FP16 mode.\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "WARNING: [TRT]: Detected invalid timing cache, setup a local cache instead\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "0:00:49.879674282 \u001b[334m 2349\u001b[00m      0x34458d0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:638:gst_nvinfer_logger:<primary-inference>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::buildModel() <nvdsinfer_context_impl.cpp:1947> [UID = 1]: serialize cuda engine to file: /dli/task/tao_project/models/resnet18_detector_final/resnet18_detector_pruned_retrained_qat.etlt_b16_gpu0_int8.engine successfully\n",
      "WARNING: [TRT]: TensorRT was linked against cuBLAS/cuBLAS LT 11.5.1 but loaded cuBLAS/cuBLAS LT 11.3.0\n",
      "WARNING: [TRT]: TensorRT was linked against cuDNN 8.2.1 but loaded cuDNN 8.1.1\n",
      "INFO: ../nvdsinfer/nvdsinfer_model_builder.cpp:610 [Implicit Engine Info]: layers num: 3\n",
      "0   INPUT  kFLOAT input_1         3x696x888       \n",
      "1   OUTPUT kFLOAT output_bbox/BiasAdd 4x44x56         \n",
      "2   OUTPUT kFLOAT output_cov/Sigmoid 1x44x56         \n",
      "\n",
      "0:00:49.886847556 \u001b[334m 2349\u001b[00m      0x34458d0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:313:notifyLoadModelStatus:<primary-inference>\u001b[00m [UID 1]: Load new model:spec_files/pgie_config_resnet18_detector_optimized.txt sucessfully\n",
      "FPS: 0.32 @ Frame 0.\n",
      "FPS: 514.82 @ Frame 100.\n",
      "FPS: 521.88 @ Frame 200.\n",
      "FPS: 615.06 @ Frame 300.\n",
      "FPS: 512.38 @ Frame 400.\n",
      "FPS: 490.16 @ Frame 500.\n",
      "FPS: 403.51 @ Frame 600.\n",
      "FPS: 478.67 @ Frame 700.\n",
      "End-of-stream--- 24.52439546585083 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Run the app_04.py DeepStream pipeline w/ the pruned ResNet18 model\n",
    "!nvidia-smi dmon -i 0 \\\n",
    "                 -s ucmt \\\n",
    "                 -c 20 > '/dli/task/logs/smi.log' & \\\n",
    "python sample_apps/app_04.py /dli/task/data/sample_30.h264 \\\n",
    "                            spec_files/pgie_config_resnet18_detector_optimized.txt \\\n",
    "                            16 \\\n",
    "                            output_tiled_optimized.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "836af9cc-fc51-4b2e-9a8a-1160794cedf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# gpu    sm   mem   enc   dec  mclk  pclk    fb  bar1 rxpci txpci\n",
      "# Idx     %     %     %     %   MHz   MHz    MB    MB  MB/s  MB/s\n",
      "    0     0     0     0     0   405   300     0     2     0     0\n",
      "    0     0     0     0     0  5000   585   280     5   522    31\n",
      "    0     5     0     0     0  5000   585   850     5     1     6\n",
      "    0    92    39     0     0  5000  1440  1114     5  2405   269\n",
      "    0    96    17     0     0  5000  1395  1114     5     3     3\n",
      "    0   100     2     0     0  5000  1500  1114     5     0     0\n",
      "    0    99     5     0     0  5000  1500  1114     5     7     4\n",
      "    0    99     6     0     0  5000  1425  1114     5    10     2\n",
      "    0    99     4     0     0  5000  1485  1114     5    12     5\n",
      "    0    94   100     0     0  5000  1410  1224     5    23     7\n",
      "    0    93    49     0     0  5000  1305  1224     5    16     5\n",
      "    0    90    37     0     0  5000  1365  1252     5    11     5\n",
      "    0    80    76     0     0  5000  1530  1252     5    27     9\n",
      "    0    92    60     0     0  5000  1200  1252     5   704   119\n",
      "    0    94    51     0     0  5000  1050  1252     5    15     3\n",
      "    0    92    37     0     0  5000  1320  1252     5    14     4\n",
      "    0    93    59     0     0  5000  1110  1252     5    17     2\n",
      "    0    93    23     0     0  5000  1290  1252     5     3   125\n",
      "    0    94    27     0     0  5000  1350  1252     5    26     1\n",
      "    0    94    29     0     0  5000  1185  1258     5     7     4\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Read the smi.log\n",
    "!cat logs/smi.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86274023-8fe8-479e-9a24-4498383baf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert the output video to a format that is compatible with Jupyter Lab\n",
    "!ffmpeg -i output_tiled_optimized.mp4 output_tiled_optimized_conv.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56b1578b-28bb-42e4-aadb-c1052f739e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"output_tiled_optimized_conv.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Show video\n",
    "Video('output_tiled_optimized_conv.mp4', width=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033a792-e622-4ce3-b3d8-2a4b86b7f3de",
   "metadata": {},
   "source": [
    "**Observations**:<br>\n",
    "The pipeline runs smoothly with the pruned model. It is memory and hardware efficient, allowing it to perform accurate, real-time video AI inference from multiple sources without noticeable latency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38589afe",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
